{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This structure is for BERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertModel, PreTrainedTokenizer, get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torch.nn.utils.rnn import pad_sequence # for dynamic padding\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(114514)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2Model(\n",
       "  (embeddings): DebertaV2Embeddings(\n",
       "    (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (encoder): DebertaV2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DebertaV2Layer(\n",
       "        (attention): DebertaV2Attention(\n",
       "          (self): DisentangledSelfAttention(\n",
       "            (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (pos_dropout): StableDropout()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "          (output): DebertaV2SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DebertaV2Intermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DebertaV2Output(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): StableDropout()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pretrained model\n",
    "model : BertModel = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "tokenizer : PreTrainedTokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8525</th>\n",
       "      <td>any enjoyment will be hinge from a personal th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8526</th>\n",
       "      <td>if legendary shlockmeister ed wood had ever ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8527</th>\n",
       "      <td>hardly a nuanced portrait of a young woman's b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8528</th>\n",
       "      <td>interminably bleak , to say nothing of boring .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8529</th>\n",
       "      <td>things really get weird , though not particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8530 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     the rock is destined to be the 21st century's ...      1\n",
       "1     the gorgeously elaborate continuation of \" the...      1\n",
       "2                        effective but too-tepid biopic      1\n",
       "3     if you sometimes like to go to the movies to h...      1\n",
       "4     emerges as something rare , an issue movie tha...      1\n",
       "...                                                 ...    ...\n",
       "8525  any enjoyment will be hinge from a personal th...      0\n",
       "8526  if legendary shlockmeister ed wood had ever ma...      0\n",
       "8527  hardly a nuanced portrait of a young woman's b...      0\n",
       "8528    interminably bleak , to say nothing of boring .      0\n",
       "8529  things really get weird , though not particula...      0\n",
       "\n",
       "[8530 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import datasets\n",
    "\n",
    "train_df = pd.read_csv(\"../preprocessed_dataset/train.csv\").iloc[:,1:]\n",
    "validation_df = pd.read_csv(\"../preprocessed_dataset/validation.csv\").iloc[:,1:]\n",
    "test_df = pd.read_csv(\"../preprocessed_dataset/test.csv\").iloc[:,1:]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 12018, 447, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            bert : BertModel\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.fc = nn.Linear(768, 1) # total 768 dim output\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids : torch.Tensor,\n",
    "            attention_mask : torch.Tensor\n",
    "        ) -> torch.Tensor:\n",
    "        outputs = self.bert.forward(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "        # get the output of [CLS] position (first position) for training \n",
    "        cls_output = outputs.last_hidden_state[:,0,:] # [B, seqlen, embed]\n",
    "        return self.fc.forward(cls_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 46733, 268, 32392, 268, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"efsdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset and data loader\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class CustomizeDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer : PreTrainedTokenizer,\n",
    "            df : pd.DataFrame\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(\n",
    "            self, \n",
    "            index : int\n",
    "        ) -> dict:\n",
    "        \n",
    "        inputs = self.df.iloc[index,0] # 0 is text\n",
    "        label = self.df.iloc[index, 1] # 1 is label\n",
    "        \n",
    "        tok = self.tokenizer(inputs)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\" : tok[\"input_ids\"],\n",
    "            \"label\" : label\n",
    "        }\n",
    "        \n",
    "# collater function\n",
    "class Collater:\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer : PreTrainedTokenizer\n",
    "        ) -> None:\n",
    "        self.tokenizer = tokenizer \n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        instances : list\n",
    "        ) -> Any:\n",
    "        input_ids = [torch.tensor(instance[\"input_ids\"], dtype = torch.int64) for instance in instances]\n",
    "        label = [torch.tensor(instance[\"label\"], dtype = torch.int64) for instance in instances]\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id).type(torch.int64)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"label\": torch.tensor(label),\n",
    "            \"attention_mask\": attention_mask # attention mask 本质就是找到不等于 pad_token_id 的位置，就是有效位置\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper paramaters\n",
    "\n",
    "num_train_epochs = 2\n",
    "\n",
    "batch_size = 16\n",
    "lr = 2e-4\n",
    "weight_decay = 1e-6\n",
    "\n",
    "warmup_ratio=0.05\n",
    "max_grad_norm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and data loader\n",
    "\n",
    "collate_fn = Collater(tokenizer)\n",
    "\n",
    "train_ds = CustomizeDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    df = train_df\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_ds,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = collate_fn,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_ds = CustomizeDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    df = validation_df\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset = val_ds,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "test_ds = CustomizeDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    df = test_df\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_ds,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:1068, warm up: 53\n"
     ]
    }
   ],
   "source": [
    "# compute warmup status\n",
    "num_training_steps = num_train_epochs * len(train_loader)\n",
    "num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "print(f\"train:{num_training_steps}, warm up: {num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer, loss_fn and so on\n",
    "\n",
    "cls_model = SentimentClassifier(model)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = AdamW(\n",
    "    params = cls_model.parameters(),\n",
    "    lr = lr,\n",
    "    weight_decay = weight_decay\n",
    ")\n",
    "\n",
    "scheduler  = get_cosine_schedule_with_warmup(\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = num_warmup_steps,\n",
    "    num_training_steps = num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "\n",
    "def compute_accuracy(data_loader: DataLoader) -> float:\n",
    "    \n",
    "    cls_model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            labels = data[\"label\"].view(-1, 1).float().to(device)\n",
    "            \n",
    "            logits = cls_model(input_ids, attention_mask=attention_mask)\n",
    "     \n",
    "            predictions = (logits > 0).long()\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1068] train loss: [0.7727] (epoch [1/2])\n",
      "[2/1068] train loss: [0.7663] (epoch [1/2])\n",
      "[3/1068] train loss: [0.6256] (epoch [1/2])\n",
      "[4/1068] train loss: [0.7094] (epoch [1/2])\n",
      "[5/1068] train loss: [0.6739] (epoch [1/2])\n",
      "[6/1068] train loss: [0.6398] (epoch [1/2])\n",
      "[7/1068] train loss: [0.6929] (epoch [1/2])\n",
      "[8/1068] train loss: [0.7200] (epoch [1/2])\n",
      "[9/1068] train loss: [0.6619] (epoch [1/2])\n",
      "[10/1068] train loss: [0.5983] (epoch [1/2])\n",
      "[11/1068] train loss: [0.7275] (epoch [1/2])\n",
      "[12/1068] train loss: [0.7151] (epoch [1/2])\n",
      "[13/1068] train loss: [0.6762] (epoch [1/2])\n",
      "[14/1068] train loss: [0.7005] (epoch [1/2])\n",
      "[15/1068] train loss: [0.6514] (epoch [1/2])\n",
      "[16/1068] train loss: [0.6439] (epoch [1/2])\n",
      "[17/1068] train loss: [0.6480] (epoch [1/2])\n",
      "[18/1068] train loss: [0.7064] (epoch [1/2])\n",
      "[19/1068] train loss: [0.7021] (epoch [1/2])\n",
      "[20/1068] train loss: [0.7008] (epoch [1/2])\n",
      "[21/1068] train loss: [0.5955] (epoch [1/2])\n",
      "[22/1068] train loss: [0.5675] (epoch [1/2])\n",
      "[23/1068] train loss: [0.5307] (epoch [1/2])\n",
      "[24/1068] train loss: [0.4567] (epoch [1/2])\n",
      "[25/1068] train loss: [0.5323] (epoch [1/2])\n",
      "[26/1068] train loss: [0.4447] (epoch [1/2])\n",
      "[27/1068] train loss: [0.7321] (epoch [1/2])\n",
      "[28/1068] train loss: [0.4376] (epoch [1/2])\n",
      "[29/1068] train loss: [1.5810] (epoch [1/2])\n",
      "[30/1068] train loss: [1.0032] (epoch [1/2])\n",
      "[31/1068] train loss: [0.9132] (epoch [1/2])\n",
      "[32/1068] train loss: [0.7194] (epoch [1/2])\n",
      "[33/1068] train loss: [0.6428] (epoch [1/2])\n",
      "[34/1068] train loss: [0.6237] (epoch [1/2])\n",
      "[35/1068] train loss: [0.6870] (epoch [1/2])\n",
      "[36/1068] train loss: [0.8088] (epoch [1/2])\n",
      "[37/1068] train loss: [0.6226] (epoch [1/2])\n",
      "[38/1068] train loss: [0.6556] (epoch [1/2])\n",
      "[39/1068] train loss: [0.5867] (epoch [1/2])\n",
      "[40/1068] train loss: [0.5426] (epoch [1/2])\n",
      "[41/1068] train loss: [0.4102] (epoch [1/2])\n",
      "[42/1068] train loss: [0.5528] (epoch [1/2])\n",
      "[43/1068] train loss: [0.4679] (epoch [1/2])\n",
      "[44/1068] train loss: [0.5958] (epoch [1/2])\n",
      "[45/1068] train loss: [0.7090] (epoch [1/2])\n",
      "[46/1068] train loss: [0.7321] (epoch [1/2])\n",
      "[47/1068] train loss: [0.8355] (epoch [1/2])\n",
      "[48/1068] train loss: [0.7675] (epoch [1/2])\n",
      "[49/1068] train loss: [0.7374] (epoch [1/2])\n",
      "[50/1068] train loss: [0.7381] (epoch [1/2])\n",
      "[51/1068] train loss: [0.6426] (epoch [1/2])\n",
      "[52/1068] train loss: [0.7447] (epoch [1/2])\n",
      "[53/1068] train loss: [0.6943] (epoch [1/2])\n",
      "[54/1068] train loss: [0.6908] (epoch [1/2])\n",
      "[55/1068] train loss: [0.6829] (epoch [1/2])\n",
      "[56/1068] train loss: [0.6284] (epoch [1/2])\n",
      "[57/1068] train loss: [0.6582] (epoch [1/2])\n",
      "[58/1068] train loss: [0.7870] (epoch [1/2])\n",
      "[59/1068] train loss: [0.7292] (epoch [1/2])\n",
      "[60/1068] train loss: [0.8510] (epoch [1/2])\n",
      "[61/1068] train loss: [0.6296] (epoch [1/2])\n",
      "[62/1068] train loss: [0.7115] (epoch [1/2])\n",
      "[63/1068] train loss: [0.6839] (epoch [1/2])\n",
      "[64/1068] train loss: [0.7278] (epoch [1/2])\n",
      "[65/1068] train loss: [0.6653] (epoch [1/2])\n",
      "[66/1068] train loss: [0.6347] (epoch [1/2])\n",
      "[67/1068] train loss: [0.6997] (epoch [1/2])\n",
      "[68/1068] train loss: [0.7101] (epoch [1/2])\n",
      "[69/1068] train loss: [0.7410] (epoch [1/2])\n",
      "[70/1068] train loss: [0.6735] (epoch [1/2])\n",
      "[71/1068] train loss: [0.6079] (epoch [1/2])\n",
      "[72/1068] train loss: [0.6897] (epoch [1/2])\n",
      "[73/1068] train loss: [0.6603] (epoch [1/2])\n",
      "[74/1068] train loss: [0.7702] (epoch [1/2])\n",
      "[75/1068] train loss: [0.7545] (epoch [1/2])\n",
      "[76/1068] train loss: [0.7013] (epoch [1/2])\n",
      "[77/1068] train loss: [0.6844] (epoch [1/2])\n",
      "[78/1068] train loss: [0.6576] (epoch [1/2])\n",
      "[79/1068] train loss: [0.7120] (epoch [1/2])\n",
      "[80/1068] train loss: [0.7487] (epoch [1/2])\n",
      "[81/1068] train loss: [0.6975] (epoch [1/2])\n",
      "[82/1068] train loss: [0.7912] (epoch [1/2])\n",
      "[83/1068] train loss: [0.6796] (epoch [1/2])\n",
      "[84/1068] train loss: [0.8509] (epoch [1/2])\n",
      "[85/1068] train loss: [0.7467] (epoch [1/2])\n",
      "[86/1068] train loss: [0.6490] (epoch [1/2])\n",
      "[87/1068] train loss: [0.7370] (epoch [1/2])\n",
      "[88/1068] train loss: [0.6658] (epoch [1/2])\n",
      "[89/1068] train loss: [0.7227] (epoch [1/2])\n",
      "[90/1068] train loss: [0.6933] (epoch [1/2])\n",
      "[91/1068] train loss: [0.6890] (epoch [1/2])\n",
      "[92/1068] train loss: [0.7059] (epoch [1/2])\n",
      "[93/1068] train loss: [0.7277] (epoch [1/2])\n",
      "[94/1068] train loss: [0.6858] (epoch [1/2])\n",
      "[95/1068] train loss: [0.7153] (epoch [1/2])\n",
      "[96/1068] train loss: [0.6747] (epoch [1/2])\n",
      "[97/1068] train loss: [0.6510] (epoch [1/2])\n",
      "[98/1068] train loss: [0.7186] (epoch [1/2])\n",
      "[99/1068] train loss: [0.7247] (epoch [1/2])\n",
      "[100/1068] train loss: [0.7145] (epoch [1/2])\n",
      "[101/1068] train loss: [0.7049] (epoch [1/2])\n",
      "[102/1068] train loss: [0.6983] (epoch [1/2])\n",
      "[103/1068] train loss: [0.6764] (epoch [1/2])\n",
      "[104/1068] train loss: [0.7341] (epoch [1/2])\n",
      "[105/1068] train loss: [0.7110] (epoch [1/2])\n",
      "[106/1068] train loss: [0.6868] (epoch [1/2])\n",
      "[107/1068] train loss: [0.6848] (epoch [1/2])\n",
      "[108/1068] train loss: [0.6998] (epoch [1/2])\n",
      "[109/1068] train loss: [0.6882] (epoch [1/2])\n",
      "[110/1068] train loss: [0.6924] (epoch [1/2])\n",
      "[111/1068] train loss: [0.6910] (epoch [1/2])\n",
      "[112/1068] train loss: [0.7795] (epoch [1/2])\n",
      "[113/1068] train loss: [0.6626] (epoch [1/2])\n",
      "[114/1068] train loss: [0.7129] (epoch [1/2])\n",
      "[115/1068] train loss: [0.6732] (epoch [1/2])\n",
      "[116/1068] train loss: [0.7056] (epoch [1/2])\n",
      "[117/1068] train loss: [0.7632] (epoch [1/2])\n",
      "[118/1068] train loss: [0.7383] (epoch [1/2])\n",
      "[119/1068] train loss: [0.6911] (epoch [1/2])\n",
      "[120/1068] train loss: [0.7244] (epoch [1/2])\n",
      "[121/1068] train loss: [0.6720] (epoch [1/2])\n",
      "[122/1068] train loss: [0.7005] (epoch [1/2])\n",
      "[123/1068] train loss: [0.6962] (epoch [1/2])\n",
      "[124/1068] train loss: [0.6998] (epoch [1/2])\n",
      "[125/1068] train loss: [0.6602] (epoch [1/2])\n",
      "[126/1068] train loss: [0.6796] (epoch [1/2])\n",
      "[127/1068] train loss: [0.7107] (epoch [1/2])\n",
      "[128/1068] train loss: [0.6595] (epoch [1/2])\n",
      "[129/1068] train loss: [0.7093] (epoch [1/2])\n",
      "[130/1068] train loss: [0.7058] (epoch [1/2])\n",
      "[131/1068] train loss: [0.6894] (epoch [1/2])\n",
      "[132/1068] train loss: [0.7293] (epoch [1/2])\n",
      "[133/1068] train loss: [0.7231] (epoch [1/2])\n",
      "[134/1068] train loss: [0.6641] (epoch [1/2])\n",
      "[135/1068] train loss: [0.6831] (epoch [1/2])\n",
      "[136/1068] train loss: [0.6987] (epoch [1/2])\n",
      "[137/1068] train loss: [0.6340] (epoch [1/2])\n",
      "[138/1068] train loss: [0.6682] (epoch [1/2])\n",
      "[139/1068] train loss: [0.6989] (epoch [1/2])\n",
      "[140/1068] train loss: [0.6879] (epoch [1/2])\n",
      "[141/1068] train loss: [0.6587] (epoch [1/2])\n",
      "[142/1068] train loss: [0.6973] (epoch [1/2])\n",
      "[143/1068] train loss: [0.7104] (epoch [1/2])\n",
      "[144/1068] train loss: [0.6676] (epoch [1/2])\n",
      "[145/1068] train loss: [0.6966] (epoch [1/2])\n",
      "[146/1068] train loss: [0.6871] (epoch [1/2])\n",
      "[147/1068] train loss: [0.7032] (epoch [1/2])\n",
      "[148/1068] train loss: [0.7014] (epoch [1/2])\n",
      "[149/1068] train loss: [0.6703] (epoch [1/2])\n",
      "[150/1068] train loss: [0.6808] (epoch [1/2])\n",
      "[151/1068] train loss: [0.7035] (epoch [1/2])\n",
      "[152/1068] train loss: [0.7161] (epoch [1/2])\n",
      "[153/1068] train loss: [0.7104] (epoch [1/2])\n",
      "[154/1068] train loss: [0.7228] (epoch [1/2])\n",
      "[155/1068] train loss: [0.6809] (epoch [1/2])\n",
      "[156/1068] train loss: [0.7081] (epoch [1/2])\n",
      "[157/1068] train loss: [0.6838] (epoch [1/2])\n",
      "[158/1068] train loss: [0.7146] (epoch [1/2])\n",
      "[159/1068] train loss: [0.7267] (epoch [1/2])\n",
      "[160/1068] train loss: [0.7441] (epoch [1/2])\n",
      "[161/1068] train loss: [0.6786] (epoch [1/2])\n",
      "[162/1068] train loss: [0.6876] (epoch [1/2])\n",
      "[163/1068] train loss: [0.7191] (epoch [1/2])\n",
      "[164/1068] train loss: [0.6940] (epoch [1/2])\n",
      "[165/1068] train loss: [0.7132] (epoch [1/2])\n",
      "[166/1068] train loss: [0.6729] (epoch [1/2])\n",
      "[167/1068] train loss: [0.6677] (epoch [1/2])\n",
      "[168/1068] train loss: [0.7050] (epoch [1/2])\n",
      "[169/1068] train loss: [0.7082] (epoch [1/2])\n",
      "[170/1068] train loss: [0.7177] (epoch [1/2])\n",
      "[171/1068] train loss: [0.7188] (epoch [1/2])\n",
      "[172/1068] train loss: [0.7015] (epoch [1/2])\n",
      "[173/1068] train loss: [0.7235] (epoch [1/2])\n",
      "[174/1068] train loss: [0.6712] (epoch [1/2])\n",
      "[175/1068] train loss: [0.7027] (epoch [1/2])\n",
      "[176/1068] train loss: [0.6768] (epoch [1/2])\n",
      "[177/1068] train loss: [0.7330] (epoch [1/2])\n",
      "[178/1068] train loss: [0.6903] (epoch [1/2])\n",
      "[179/1068] train loss: [0.6806] (epoch [1/2])\n",
      "[180/1068] train loss: [0.7336] (epoch [1/2])\n",
      "[181/1068] train loss: [0.7174] (epoch [1/2])\n",
      "[182/1068] train loss: [0.7165] (epoch [1/2])\n",
      "[183/1068] train loss: [0.7064] (epoch [1/2])\n",
      "[184/1068] train loss: [0.6821] (epoch [1/2])\n",
      "[185/1068] train loss: [0.7346] (epoch [1/2])\n",
      "[186/1068] train loss: [0.7202] (epoch [1/2])\n",
      "[187/1068] train loss: [0.7069] (epoch [1/2])\n",
      "[188/1068] train loss: [0.6865] (epoch [1/2])\n",
      "[189/1068] train loss: [0.6733] (epoch [1/2])\n",
      "[190/1068] train loss: [0.7036] (epoch [1/2])\n",
      "[191/1068] train loss: [0.6978] (epoch [1/2])\n",
      "[192/1068] train loss: [0.7050] (epoch [1/2])\n",
      "[193/1068] train loss: [0.6790] (epoch [1/2])\n",
      "[194/1068] train loss: [0.6822] (epoch [1/2])\n",
      "[195/1068] train loss: [0.6617] (epoch [1/2])\n",
      "[196/1068] train loss: [0.7018] (epoch [1/2])\n",
      "[197/1068] train loss: [0.7035] (epoch [1/2])\n",
      "[198/1068] train loss: [0.6808] (epoch [1/2])\n",
      "[199/1068] train loss: [0.7400] (epoch [1/2])\n",
      "[200/1068] train loss: [0.6829] (epoch [1/2])\n",
      "[201/1068] train loss: [0.7260] (epoch [1/2])\n",
      "[202/1068] train loss: [0.7030] (epoch [1/2])\n",
      "[203/1068] train loss: [0.6798] (epoch [1/2])\n",
      "[204/1068] train loss: [0.7416] (epoch [1/2])\n",
      "[205/1068] train loss: [0.7337] (epoch [1/2])\n",
      "[206/1068] train loss: [0.6969] (epoch [1/2])\n",
      "[207/1068] train loss: [0.7098] (epoch [1/2])\n",
      "[208/1068] train loss: [0.6869] (epoch [1/2])\n",
      "[209/1068] train loss: [0.7342] (epoch [1/2])\n",
      "[210/1068] train loss: [0.7199] (epoch [1/2])\n",
      "[211/1068] train loss: [0.6883] (epoch [1/2])\n",
      "[212/1068] train loss: [0.7193] (epoch [1/2])\n",
      "[213/1068] train loss: [0.6630] (epoch [1/2])\n",
      "[214/1068] train loss: [0.6546] (epoch [1/2])\n",
      "[215/1068] train loss: [0.6852] (epoch [1/2])\n",
      "[216/1068] train loss: [0.6777] (epoch [1/2])\n",
      "[217/1068] train loss: [0.6971] (epoch [1/2])\n",
      "[218/1068] train loss: [0.7028] (epoch [1/2])\n",
      "[219/1068] train loss: [0.6728] (epoch [1/2])\n",
      "[220/1068] train loss: [0.6895] (epoch [1/2])\n",
      "[221/1068] train loss: [0.6871] (epoch [1/2])\n",
      "[222/1068] train loss: [0.6933] (epoch [1/2])\n",
      "[223/1068] train loss: [0.6971] (epoch [1/2])\n",
      "[224/1068] train loss: [0.7145] (epoch [1/2])\n",
      "[225/1068] train loss: [0.7215] (epoch [1/2])\n",
      "[226/1068] train loss: [0.7166] (epoch [1/2])\n",
      "[227/1068] train loss: [0.7071] (epoch [1/2])\n",
      "[228/1068] train loss: [0.6906] (epoch [1/2])\n",
      "[229/1068] train loss: [0.6785] (epoch [1/2])\n",
      "[230/1068] train loss: [0.7015] (epoch [1/2])\n",
      "[231/1068] train loss: [0.7303] (epoch [1/2])\n",
      "[232/1068] train loss: [0.7080] (epoch [1/2])\n",
      "[233/1068] train loss: [0.7205] (epoch [1/2])\n",
      "[234/1068] train loss: [0.7058] (epoch [1/2])\n",
      "[235/1068] train loss: [0.6819] (epoch [1/2])\n",
      "[236/1068] train loss: [0.7085] (epoch [1/2])\n",
      "[237/1068] train loss: [0.6974] (epoch [1/2])\n",
      "[238/1068] train loss: [0.7130] (epoch [1/2])\n",
      "[239/1068] train loss: [0.7105] (epoch [1/2])\n",
      "[240/1068] train loss: [0.7023] (epoch [1/2])\n",
      "[241/1068] train loss: [0.7229] (epoch [1/2])\n",
      "[242/1068] train loss: [0.7298] (epoch [1/2])\n",
      "[243/1068] train loss: [0.7623] (epoch [1/2])\n",
      "[244/1068] train loss: [0.7289] (epoch [1/2])\n",
      "[245/1068] train loss: [0.7599] (epoch [1/2])\n",
      "[246/1068] train loss: [0.6325] (epoch [1/2])\n",
      "[247/1068] train loss: [0.6806] (epoch [1/2])\n",
      "[248/1068] train loss: [0.7035] (epoch [1/2])\n",
      "[249/1068] train loss: [0.6941] (epoch [1/2])\n",
      "[250/1068] train loss: [0.6989] (epoch [1/2])\n",
      "[251/1068] train loss: [0.6954] (epoch [1/2])\n",
      "[252/1068] train loss: [0.6903] (epoch [1/2])\n",
      "[253/1068] train loss: [0.7059] (epoch [1/2])\n",
      "[254/1068] train loss: [0.6942] (epoch [1/2])\n",
      "[255/1068] train loss: [0.7040] (epoch [1/2])\n",
      "[256/1068] train loss: [0.7301] (epoch [1/2])\n",
      "[257/1068] train loss: [0.6818] (epoch [1/2])\n",
      "[258/1068] train loss: [0.7024] (epoch [1/2])\n",
      "[259/1068] train loss: [0.7200] (epoch [1/2])\n",
      "[260/1068] train loss: [0.7049] (epoch [1/2])\n",
      "[261/1068] train loss: [0.7023] (epoch [1/2])\n",
      "[262/1068] train loss: [0.6978] (epoch [1/2])\n",
      "[263/1068] train loss: [0.6912] (epoch [1/2])\n",
      "[264/1068] train loss: [0.6972] (epoch [1/2])\n",
      "[265/1068] train loss: [0.6806] (epoch [1/2])\n",
      "[266/1068] train loss: [0.7673] (epoch [1/2])\n",
      "[267/1068] train loss: [0.6923] (epoch [1/2])\n",
      "[268/1068] train loss: [0.6988] (epoch [1/2])\n",
      "[269/1068] train loss: [0.6632] (epoch [1/2])\n",
      "[270/1068] train loss: [0.7103] (epoch [1/2])\n",
      "[271/1068] train loss: [0.7084] (epoch [1/2])\n",
      "[272/1068] train loss: [0.6882] (epoch [1/2])\n",
      "[273/1068] train loss: [0.6757] (epoch [1/2])\n",
      "[274/1068] train loss: [0.7442] (epoch [1/2])\n",
      "[275/1068] train loss: [0.6912] (epoch [1/2])\n",
      "[276/1068] train loss: [0.6721] (epoch [1/2])\n",
      "[277/1068] train loss: [0.6893] (epoch [1/2])\n",
      "[278/1068] train loss: [0.6702] (epoch [1/2])\n",
      "[279/1068] train loss: [0.6802] (epoch [1/2])\n",
      "[280/1068] train loss: [0.7055] (epoch [1/2])\n",
      "[281/1068] train loss: [0.6746] (epoch [1/2])\n",
      "[282/1068] train loss: [0.6846] (epoch [1/2])\n",
      "[283/1068] train loss: [0.6404] (epoch [1/2])\n",
      "[284/1068] train loss: [0.7142] (epoch [1/2])\n",
      "[285/1068] train loss: [0.7206] (epoch [1/2])\n",
      "[286/1068] train loss: [0.7853] (epoch [1/2])\n",
      "[287/1068] train loss: [0.8293] (epoch [1/2])\n",
      "[288/1068] train loss: [0.6843] (epoch [1/2])\n",
      "[289/1068] train loss: [0.6957] (epoch [1/2])\n",
      "[290/1068] train loss: [0.7522] (epoch [1/2])\n",
      "[291/1068] train loss: [0.7192] (epoch [1/2])\n",
      "[292/1068] train loss: [0.6780] (epoch [1/2])\n",
      "[293/1068] train loss: [0.6980] (epoch [1/2])\n",
      "[294/1068] train loss: [0.6974] (epoch [1/2])\n",
      "[295/1068] train loss: [0.7588] (epoch [1/2])\n",
      "[296/1068] train loss: [0.7197] (epoch [1/2])\n",
      "[297/1068] train loss: [0.7021] (epoch [1/2])\n",
      "[298/1068] train loss: [0.6914] (epoch [1/2])\n",
      "[299/1068] train loss: [0.6939] (epoch [1/2])\n",
      "[300/1068] train loss: [0.6731] (epoch [1/2])\n",
      "[301/1068] train loss: [0.6948] (epoch [1/2])\n",
      "[302/1068] train loss: [0.6491] (epoch [1/2])\n",
      "[303/1068] train loss: [0.6926] (epoch [1/2])\n",
      "[304/1068] train loss: [0.6837] (epoch [1/2])\n",
      "[305/1068] train loss: [0.6846] (epoch [1/2])\n",
      "[306/1068] train loss: [0.6559] (epoch [1/2])\n",
      "[307/1068] train loss: [0.6339] (epoch [1/2])\n",
      "[308/1068] train loss: [0.6819] (epoch [1/2])\n",
      "[309/1068] train loss: [0.6411] (epoch [1/2])\n",
      "[310/1068] train loss: [0.7271] (epoch [1/2])\n",
      "[311/1068] train loss: [0.7435] (epoch [1/2])\n",
      "[312/1068] train loss: [0.7752] (epoch [1/2])\n",
      "[313/1068] train loss: [0.7541] (epoch [1/2])\n",
      "[314/1068] train loss: [0.6962] (epoch [1/2])\n",
      "[315/1068] train loss: [0.7745] (epoch [1/2])\n",
      "[316/1068] train loss: [0.7345] (epoch [1/2])\n",
      "[317/1068] train loss: [0.7002] (epoch [1/2])\n",
      "[318/1068] train loss: [0.6977] (epoch [1/2])\n",
      "[319/1068] train loss: [0.6793] (epoch [1/2])\n",
      "[320/1068] train loss: [0.6906] (epoch [1/2])\n",
      "[321/1068] train loss: [0.7068] (epoch [1/2])\n",
      "[322/1068] train loss: [0.6904] (epoch [1/2])\n",
      "[323/1068] train loss: [0.6890] (epoch [1/2])\n",
      "[324/1068] train loss: [0.6750] (epoch [1/2])\n",
      "[325/1068] train loss: [0.7525] (epoch [1/2])\n",
      "[326/1068] train loss: [0.6469] (epoch [1/2])\n",
      "[327/1068] train loss: [0.7207] (epoch [1/2])\n",
      "[328/1068] train loss: [0.8192] (epoch [1/2])\n",
      "[329/1068] train loss: [0.6849] (epoch [1/2])\n",
      "[330/1068] train loss: [0.6592] (epoch [1/2])\n",
      "[331/1068] train loss: [0.6867] (epoch [1/2])\n",
      "[332/1068] train loss: [0.7164] (epoch [1/2])\n",
      "[333/1068] train loss: [0.7078] (epoch [1/2])\n",
      "[334/1068] train loss: [0.6924] (epoch [1/2])\n",
      "[335/1068] train loss: [0.6758] (epoch [1/2])\n",
      "[336/1068] train loss: [0.6750] (epoch [1/2])\n",
      "[337/1068] train loss: [0.6817] (epoch [1/2])\n",
      "[338/1068] train loss: [0.7072] (epoch [1/2])\n",
      "[339/1068] train loss: [0.6681] (epoch [1/2])\n",
      "[340/1068] train loss: [0.6826] (epoch [1/2])\n",
      "[341/1068] train loss: [0.7068] (epoch [1/2])\n",
      "[342/1068] train loss: [0.7037] (epoch [1/2])\n",
      "[343/1068] train loss: [0.6796] (epoch [1/2])\n",
      "[344/1068] train loss: [0.7149] (epoch [1/2])\n",
      "[345/1068] train loss: [0.7187] (epoch [1/2])\n",
      "[346/1068] train loss: [0.7149] (epoch [1/2])\n",
      "[347/1068] train loss: [0.7076] (epoch [1/2])\n",
      "[348/1068] train loss: [0.6982] (epoch [1/2])\n",
      "[349/1068] train loss: [0.7285] (epoch [1/2])\n",
      "[350/1068] train loss: [0.6957] (epoch [1/2])\n",
      "[351/1068] train loss: [0.6707] (epoch [1/2])\n",
      "[352/1068] train loss: [0.6986] (epoch [1/2])\n",
      "[353/1068] train loss: [0.6977] (epoch [1/2])\n",
      "[354/1068] train loss: [0.6754] (epoch [1/2])\n",
      "[355/1068] train loss: [0.6864] (epoch [1/2])\n",
      "[356/1068] train loss: [0.7102] (epoch [1/2])\n",
      "[357/1068] train loss: [0.6860] (epoch [1/2])\n",
      "[358/1068] train loss: [0.6404] (epoch [1/2])\n",
      "[359/1068] train loss: [0.7457] (epoch [1/2])\n",
      "[360/1068] train loss: [0.7333] (epoch [1/2])\n",
      "[361/1068] train loss: [0.6950] (epoch [1/2])\n",
      "[362/1068] train loss: [0.6733] (epoch [1/2])\n",
      "[363/1068] train loss: [0.6465] (epoch [1/2])\n",
      "[364/1068] train loss: [0.6579] (epoch [1/2])\n",
      "[365/1068] train loss: [0.7432] (epoch [1/2])\n",
      "[366/1068] train loss: [0.7233] (epoch [1/2])\n",
      "[367/1068] train loss: [0.7250] (epoch [1/2])\n",
      "[368/1068] train loss: [0.7014] (epoch [1/2])\n",
      "[369/1068] train loss: [0.7479] (epoch [1/2])\n",
      "[370/1068] train loss: [0.7196] (epoch [1/2])\n",
      "[371/1068] train loss: [0.6979] (epoch [1/2])\n",
      "[372/1068] train loss: [0.7030] (epoch [1/2])\n",
      "[373/1068] train loss: [0.6737] (epoch [1/2])\n",
      "[374/1068] train loss: [0.6835] (epoch [1/2])\n",
      "[375/1068] train loss: [0.6848] (epoch [1/2])\n",
      "[376/1068] train loss: [0.6929] (epoch [1/2])\n",
      "[377/1068] train loss: [0.7200] (epoch [1/2])\n",
      "[378/1068] train loss: [0.7066] (epoch [1/2])\n",
      "[379/1068] train loss: [0.6897] (epoch [1/2])\n",
      "[380/1068] train loss: [0.7175] (epoch [1/2])\n",
      "[381/1068] train loss: [0.7279] (epoch [1/2])\n",
      "[382/1068] train loss: [0.6981] (epoch [1/2])\n",
      "[383/1068] train loss: [0.6733] (epoch [1/2])\n",
      "[384/1068] train loss: [0.7037] (epoch [1/2])\n",
      "[385/1068] train loss: [0.7028] (epoch [1/2])\n",
      "[386/1068] train loss: [0.7013] (epoch [1/2])\n",
      "[387/1068] train loss: [0.7164] (epoch [1/2])\n",
      "[388/1068] train loss: [0.6775] (epoch [1/2])\n",
      "[389/1068] train loss: [0.7092] (epoch [1/2])\n",
      "[390/1068] train loss: [0.7172] (epoch [1/2])\n",
      "[391/1068] train loss: [0.6853] (epoch [1/2])\n",
      "[392/1068] train loss: [0.7125] (epoch [1/2])\n",
      "[393/1068] train loss: [0.7296] (epoch [1/2])\n",
      "[394/1068] train loss: [0.6792] (epoch [1/2])\n",
      "[395/1068] train loss: [0.7238] (epoch [1/2])\n",
      "[396/1068] train loss: [0.7001] (epoch [1/2])\n",
      "[397/1068] train loss: [0.6878] (epoch [1/2])\n",
      "[398/1068] train loss: [0.6919] (epoch [1/2])\n",
      "[399/1068] train loss: [0.6828] (epoch [1/2])\n",
      "[400/1068] train loss: [0.7022] (epoch [1/2])\n",
      "[401/1068] train loss: [0.7289] (epoch [1/2])\n",
      "[402/1068] train loss: [0.6875] (epoch [1/2])\n",
      "[403/1068] train loss: [0.6900] (epoch [1/2])\n",
      "[404/1068] train loss: [0.6775] (epoch [1/2])\n",
      "[405/1068] train loss: [0.6872] (epoch [1/2])\n",
      "[406/1068] train loss: [0.6919] (epoch [1/2])\n",
      "[407/1068] train loss: [0.6776] (epoch [1/2])\n",
      "[408/1068] train loss: [0.7174] (epoch [1/2])\n",
      "[409/1068] train loss: [0.6974] (epoch [1/2])\n",
      "[410/1068] train loss: [0.7153] (epoch [1/2])\n",
      "[411/1068] train loss: [0.7030] (epoch [1/2])\n",
      "[412/1068] train loss: [0.6941] (epoch [1/2])\n",
      "[413/1068] train loss: [0.6867] (epoch [1/2])\n",
      "[414/1068] train loss: [0.7155] (epoch [1/2])\n",
      "[415/1068] train loss: [0.6880] (epoch [1/2])\n",
      "[416/1068] train loss: [0.6974] (epoch [1/2])\n",
      "[417/1068] train loss: [0.6958] (epoch [1/2])\n",
      "[418/1068] train loss: [0.7049] (epoch [1/2])\n",
      "[419/1068] train loss: [0.6873] (epoch [1/2])\n",
      "[420/1068] train loss: [0.7269] (epoch [1/2])\n",
      "[421/1068] train loss: [0.6808] (epoch [1/2])\n",
      "[422/1068] train loss: [0.6966] (epoch [1/2])\n",
      "[423/1068] train loss: [0.6982] (epoch [1/2])\n",
      "[424/1068] train loss: [0.7126] (epoch [1/2])\n",
      "[425/1068] train loss: [0.6983] (epoch [1/2])\n",
      "[426/1068] train loss: [0.7026] (epoch [1/2])\n",
      "[427/1068] train loss: [0.6802] (epoch [1/2])\n",
      "[428/1068] train loss: [0.6906] (epoch [1/2])\n",
      "[429/1068] train loss: [0.6811] (epoch [1/2])\n",
      "[430/1068] train loss: [0.7007] (epoch [1/2])\n",
      "[431/1068] train loss: [0.6967] (epoch [1/2])\n",
      "[432/1068] train loss: [0.7083] (epoch [1/2])\n",
      "[433/1068] train loss: [0.6990] (epoch [1/2])\n",
      "[434/1068] train loss: [0.6835] (epoch [1/2])\n",
      "[435/1068] train loss: [0.6932] (epoch [1/2])\n",
      "[436/1068] train loss: [0.6896] (epoch [1/2])\n",
      "[437/1068] train loss: [0.6956] (epoch [1/2])\n",
      "[438/1068] train loss: [0.7491] (epoch [1/2])\n",
      "[439/1068] train loss: [0.7107] (epoch [1/2])\n",
      "[440/1068] train loss: [0.6846] (epoch [1/2])\n",
      "[441/1068] train loss: [0.7042] (epoch [1/2])\n",
      "[442/1068] train loss: [0.6843] (epoch [1/2])\n",
      "[443/1068] train loss: [0.6644] (epoch [1/2])\n",
      "[444/1068] train loss: [0.6801] (epoch [1/2])\n",
      "[445/1068] train loss: [0.6913] (epoch [1/2])\n",
      "[446/1068] train loss: [0.6768] (epoch [1/2])\n",
      "[447/1068] train loss: [0.6927] (epoch [1/2])\n",
      "[448/1068] train loss: [0.7465] (epoch [1/2])\n",
      "[449/1068] train loss: [0.7154] (epoch [1/2])\n",
      "[450/1068] train loss: [0.8271] (epoch [1/2])\n",
      "[451/1068] train loss: [0.7814] (epoch [1/2])\n",
      "[452/1068] train loss: [0.6911] (epoch [1/2])\n",
      "[453/1068] train loss: [0.6566] (epoch [1/2])\n",
      "[454/1068] train loss: [0.7151] (epoch [1/2])\n",
      "[455/1068] train loss: [0.7172] (epoch [1/2])\n",
      "[456/1068] train loss: [0.7516] (epoch [1/2])\n",
      "[457/1068] train loss: [0.7386] (epoch [1/2])\n",
      "[458/1068] train loss: [0.6662] (epoch [1/2])\n",
      "[459/1068] train loss: [0.6867] (epoch [1/2])\n",
      "[460/1068] train loss: [0.7002] (epoch [1/2])\n",
      "[461/1068] train loss: [0.6934] (epoch [1/2])\n",
      "[462/1068] train loss: [0.6737] (epoch [1/2])\n",
      "[463/1068] train loss: [0.7181] (epoch [1/2])\n",
      "[464/1068] train loss: [0.6772] (epoch [1/2])\n",
      "[465/1068] train loss: [0.7057] (epoch [1/2])\n",
      "[466/1068] train loss: [0.6737] (epoch [1/2])\n",
      "[467/1068] train loss: [0.7228] (epoch [1/2])\n",
      "[468/1068] train loss: [0.6952] (epoch [1/2])\n",
      "[469/1068] train loss: [0.7239] (epoch [1/2])\n",
      "[470/1068] train loss: [0.7170] (epoch [1/2])\n",
      "[471/1068] train loss: [0.7223] (epoch [1/2])\n",
      "[472/1068] train loss: [0.6993] (epoch [1/2])\n",
      "[473/1068] train loss: [0.7020] (epoch [1/2])\n",
      "[474/1068] train loss: [0.6847] (epoch [1/2])\n",
      "[475/1068] train loss: [0.7083] (epoch [1/2])\n",
      "[476/1068] train loss: [0.7146] (epoch [1/2])\n",
      "[477/1068] train loss: [0.7049] (epoch [1/2])\n",
      "[478/1068] train loss: [0.6837] (epoch [1/2])\n",
      "[479/1068] train loss: [0.6600] (epoch [1/2])\n",
      "[480/1068] train loss: [0.6743] (epoch [1/2])\n",
      "[481/1068] train loss: [0.7108] (epoch [1/2])\n",
      "[482/1068] train loss: [0.6927] (epoch [1/2])\n",
      "[483/1068] train loss: [0.6921] (epoch [1/2])\n",
      "[484/1068] train loss: [0.6527] (epoch [1/2])\n",
      "[485/1068] train loss: [0.6738] (epoch [1/2])\n",
      "[486/1068] train loss: [0.6813] (epoch [1/2])\n",
      "[487/1068] train loss: [0.7075] (epoch [1/2])\n",
      "[488/1068] train loss: [0.6951] (epoch [1/2])\n",
      "[489/1068] train loss: [0.7491] (epoch [1/2])\n",
      "[490/1068] train loss: [0.7049] (epoch [1/2])\n",
      "[491/1068] train loss: [0.7189] (epoch [1/2])\n",
      "[492/1068] train loss: [0.6413] (epoch [1/2])\n",
      "[493/1068] train loss: [0.6927] (epoch [1/2])\n",
      "[494/1068] train loss: [0.7054] (epoch [1/2])\n",
      "[495/1068] train loss: [0.6984] (epoch [1/2])\n",
      "[496/1068] train loss: [0.6928] (epoch [1/2])\n",
      "[497/1068] train loss: [0.6779] (epoch [1/2])\n",
      "[498/1068] train loss: [0.7262] (epoch [1/2])\n",
      "[499/1068] train loss: [0.6897] (epoch [1/2])\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "cls_model.to(device)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "step = 0\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    \n",
    "    # train loop\n",
    "    cls_model.train()\n",
    "    for data in train_loader:\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        attention_mask = data[\"attention_mask\"].to(device)\n",
    "        label = data[\"label\"].view(-1, 1).float().to(device)\n",
    "        \n",
    "        logits = cls_model.forward(input_ids, attention_mask)\n",
    "        \n",
    "        \n",
    "        loss = loss_fn.forward(\n",
    "            input = logits,\n",
    "            target = label\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step() # update paramater\n",
    "        scheduler.step() \n",
    "        optimizer.zero_grad() # clear gradient\n",
    "        \n",
    "        vis_loss = loss.detach().cpu()\n",
    "        \n",
    "        print(f\"[{step + 1}/{num_training_steps}] train loss: [{vis_loss:.4f}] (epoch [{epoch + 1}/{num_train_epochs}])\")\n",
    "        \n",
    "        \n",
    "        train_losses.append(vis_loss)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    val_loss = 0\n",
    "    \n",
    "    cls_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            labels = data[\"label\"].view(-1, 1).float().to(device)\n",
    "            \n",
    "            logits = cls_model.forward(input_ids, attention_mask)\n",
    "            loss = loss_fn.forward(\n",
    "                input = logits,\n",
    "                target = labels\n",
    "            )\n",
    "            \n",
    "            val_loss += loss.detach().cpu()\n",
    "            \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "    val_acc = compute_accuracy(val_loader)\n",
    "    print(f\"epoch [{epoch + 1}/{num_train_epochs}] validation loss: [{val_loss:.4f}] validation accuracy: [{val_acc}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.3094753324985504]\n",
      "Test accuracy: [0.8930581613508443]\n"
     ]
    }
   ],
   "source": [
    "# test loss\n",
    "cls_model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        attention_mask = data[\"attention_mask\"].to(device)\n",
    "        label = data[\"label\"].view(-1, 1).float().to(device)\n",
    "        \n",
    "        logits = cls_model.forward(input_ids, attention_mask)\n",
    "        loss = loss_fn.forward(\n",
    "            input = logits,\n",
    "            target = label\n",
    "        )\n",
    "        \n",
    "        test_loss += loss.detach().cpu()\n",
    "        \n",
    "    test_loss /= len(val_loader)\n",
    "    \n",
    "test_acc = compute_accuracy(test_loader)\n",
    "\n",
    "print(f\"Test Loss: [{test_loss:.4f}]\\nTest accuracy: [{test_acc:.4f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
