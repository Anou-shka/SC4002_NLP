{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c87c6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beaa37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "#GLOVE_PATH = 'C:\\\\Users\\\\Lee Ming Jia\\\\Desktop\\\\GloVe\\\\glove.6B.100d.txt'\n",
    "glove_embeddings = load_glove_embeddings('C:\\\\Users\\\\Lee Ming Jia\\\\Desktop\\\\GloVe\\\\glove.6B.100d.txt')\n",
    "embedding_dim = 100  # GloVe 100D embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2bd721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spaCy for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token.text.lower() for token in nlp(text) if not token.is_punct and not token.is_stop]\n",
    "\n",
    "def pad_sequence(sequence, max_len):\n",
    "    return sequence + ['<pad>'] * (max_len - len(sequence))\n",
    "\n",
    "def preprocess_data(texts, labels, max_len):\n",
    "    tokenized_texts = [tokenize(text) for text in texts]\n",
    "    padded_texts = [pad_sequence(text, max_len) for text in tokenized_texts]\n",
    "    return padded_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10fca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Convert text to indices using the vocab\n",
    "        indices = [self.vocab.get(word, self.vocab['<unk>']) for word in text]\n",
    "        return torch.tensor(indices), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b269f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, glove_embeddings, max_vocab_size=20000):\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            if word not in vocab and len(vocab) < max_vocab_size:\n",
    "                vocab[word] = len(vocab)\n",
    "    # Add GloVe embeddings for words in the vocab\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim))\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "    return vocab, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c37d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix))  # Initialize with pre-trained GloVe vectors\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embed_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    " #   def forward(self, text):\n",
    " #       embedded = self.embedding(text).unsqueeze(1)  # Add channel dimension\n",
    "  #      conved = [torch.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "  #      pooled = [torch.max(conv, dim=2)[0] for conv in conved]\n",
    "  #      cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "  #      return self.fc(cat)\n",
    "    \n",
    "    \n",
    "    def forward(self, text):\n",
    "        # Ensure indices are within the valid range\n",
    "        max_vocab_index = len(self.embedding.weight) - 1\n",
    "        text = torch.clamp(text, max=max_vocab_index)  # Clamp indices to the valid range\n",
    "\n",
    "        # Apply embedding\n",
    "        embedded = self.embedding(text).unsqueeze(1)  # Add channel dimension\n",
    "        conved = [torch.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [torch.max(conv, dim=2)[0] for conv in conved]\n",
    "\n",
    "        # Concatenate pooled features and pass through fully connected layer\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        output = self.fc(cat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dff9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "\n",
    "def load_contractions(file_path):\n",
    "    contractions = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                contraction, expansion = line.split(\":\")\n",
    "                contraction = contraction.strip()\n",
    "                expansion = [word.strip() for word in expansion.split(\",\")]\n",
    "                contractions[contraction] = expansion\n",
    "    return contractions\n",
    "\n",
    "def normalize_words(text): # stupid but works\n",
    "    words = []\n",
    "    contractions = load_contractions(\"contractions.txt\")\n",
    "    text = unidecode.unidecode(text) # e.g. \"café\" -> \"cafe\"\n",
    "    text = text.lower()\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = re.sub(rf\"\\b{contraction}\\b\", \" \".join(expansion), text) # e.g. \"I'm\" -> \"I am\"\n",
    "    text = re.sub(r\"[`\\[\\]\\\"]\", \"\", text) # e.g. \"`rock[n]roll`\" -> \"rocknroll\"\n",
    "    text = re.sub(r\"'s(?=[^a-zA-Z]|$)\", r\" 's \", text) # e.g. \"John's\" -> \"John 's\"\n",
    "    text = re.sub(r\"s'(?=[^a-zA-Z]|$)\", r\"s 's \", text) # e.g. \"dogs'\" -> \"dogs 's\"\n",
    "    text = re.sub(r\"[/-]\", \" \", text) # e.g. \"rock-n-roll\" -> \"rock n roll\"\n",
    "    text = re.sub(r\"([@#&%+:,.?!$€£¥\\(\\)])\", r\" \\1 \", text) # e.g. \"rock&roll\" -> \"rock & roll\"\n",
    "    text = re.sub(r\"(?<!s)'(?!s\\b\\s|s$)\", \"\", text) # e.g. \"rock'n'roll\" -> \"rocknroll\"\n",
    "    for word in text.split():\n",
    "        words.append(word)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ea01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "\n",
    "class GloveTokenizer:\n",
    "    def __init__(self, glove_file_path, pad_token=\"<PAD>\", unk_token=\"<UNK>\"):\n",
    "        # Load GloVe embeddings from file\n",
    "        self.embeddings_index = self._load_glove_embeddings(glove_file_path)\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_token_id = 1\n",
    "\n",
    "        # Create word-to-index and index-to-word dictionaries\n",
    "        self.word_index = {word: idx for idx, word in enumerate(self.embeddings_index.keys(), start=2)}\n",
    "        self.word_index[self.pad_token] = self.pad_token_id\n",
    "        self.word_index[self.unk_token] = self.unk_token_id\n",
    "        self.index_word = {idx: word for word, idx in self.word_index.items()}\n",
    "\n",
    "    def _load_glove_embeddings(self, glove_file_path):\n",
    "        # Load the GloVe embeddings from file into a dictionary\n",
    "        embeddings_index = {}\n",
    "        with open(glove_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = vector\n",
    "        return embeddings_index\n",
    "\n",
    "    def _tokenize_with_subwords(self, word):\n",
    "        # If word exists in vocabulary, return its index\n",
    "        if word in self.word_index:\n",
    "            return [self.word_index[word]]\n",
    "\n",
    "        # Otherwise, tokenize word into subwords and try to match each subword\n",
    "        subword_tokens = []\n",
    "        subwords = normalize_words(word)\n",
    "        for subword in subwords:\n",
    "            if subword in self.word_index:\n",
    "                subword_tokens.append(self.word_index[subword])\n",
    "\n",
    "        # Return indices of matched subwords, or UNK token if no subwords matched\n",
    "        if subword_tokens:\n",
    "            return subword_tokens\n",
    "        else:\n",
    "            return [self.unk_token_id]\n",
    "\n",
    "    def encode(self, texts, max_length=None, return_tensors=\"list\"):\n",
    "        # Convert single string to list for consistent handling\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        # Tokenize and encode each text in batch\n",
    "        all_input_ids = []\n",
    "        for text in texts:\n",
    "            input_ids = []\n",
    "            for word in text.split():\n",
    "                input_ids.extend(self._tokenize_with_subwords(word))\n",
    "            all_input_ids.append(input_ids)\n",
    "\n",
    "        # Pad sequences and create attention masks\n",
    "        input_ids_padded = pad_sequence(\n",
    "            [torch.tensor(seq, dtype=torch.long) for seq in all_input_ids],\n",
    "            batch_first=True,\n",
    "            padding_value=self.pad_token_id\n",
    "        )\n",
    "\n",
    "        attention_mask = (input_ids_padded != self.pad_token_id).long()\n",
    "\n",
    "        # Trim/pad to max_length if specified\n",
    "        if max_length:\n",
    "            input_ids_padded = input_ids_padded[:, :max_length]\n",
    "            attention_mask = attention_mask[:, :max_length]\n",
    "\n",
    "            if input_ids_padded.shape[1] < max_length:\n",
    "                pad_size = max_length - input_ids_padded.shape[1]\n",
    "                padding = torch.full((input_ids_padded.shape[0], pad_size), self.pad_token_id, dtype=torch.long)\n",
    "                input_ids_padded = torch.cat([input_ids_padded, padding], dim=1)\n",
    "                \n",
    "                mask_padding = torch.zeros((attention_mask.shape[0], pad_size), dtype=torch.long)\n",
    "                attention_mask = torch.cat([attention_mask, mask_padding], dim=1)\n",
    "\n",
    "        # Return in specified format\n",
    "        if return_tensors == \"pt\":\n",
    "            return {\"input_ids\": input_ids_padded, \"attention_mask\": attention_mask}\n",
    "        else:\n",
    "            return {\"input_ids\": input_ids_padded.tolist(), \"attention_mask\": attention_mask.tolist()}\n",
    "\n",
    "    def decode(self, token_ids_batch, skip_special_tokens=False):\n",
    "        # Convert tensor to list for consistent handling\n",
    "        if isinstance(token_ids_batch, torch.Tensor):\n",
    "            if token_ids_batch.dim() != 2:\n",
    "                raise ValueError(\"Input tensor must be 2-dimensional.\")\n",
    "            token_ids_batch = token_ids_batch.tolist()\n",
    "\n",
    "        # Ensure batch format\n",
    "        if isinstance(token_ids_batch[0], int):\n",
    "            token_ids_batch = [token_ids_batch]\n",
    "\n",
    "        # Decode each sequence in batch\n",
    "        all_texts = []\n",
    "        for token_ids in token_ids_batch:\n",
    "            words = []\n",
    "            for idx in token_ids:\n",
    "                word = self.index_word.get(idx, self.unk_token)\n",
    "                if skip_special_tokens and word == self.pad_token:\n",
    "                    continue\n",
    "                words.append(word)\n",
    "            all_texts.append(\" \".join(words))\n",
    "\n",
    "        # Return single or list of decoded texts\n",
    "        return all_texts if len(all_texts) > 1 else all_texts[0]\n",
    "\n",
    "class GloveTokenizerNoSub(GloveTokenizer):\n",
    "    def __init__(self, glove_file_path, pad_token=\"<PAD>\", unk_token=\"<UNK>\"):\n",
    "        super().__init__(glove_file_path, pad_token, unk_token)\n",
    "\n",
    "    def _tokenize_with_subwords(self, word):\n",
    "        if word in self.word_index:\n",
    "            return [self.word_index[word]]\n",
    "        else:\n",
    "            return [self.unk_token_id]\n",
    "\n",
    "class GloveEmbedding(nn.Module):\n",
    "    def __init__(self, glove_file_path, embedding_dim=100, trainable=False, pad_token=\"<PAD>\", unk_token=\"<UNK>\"):\n",
    "        super(GloveEmbedding, self).__init__()\n",
    "        # Initialize vocabulary and embedding matrix with GloVe embeddings\n",
    "        self.word_index, embedding_matrix = self._load_glove_embeddings(glove_file_path, embedding_dim, pad_token, unk_token)\n",
    "        \n",
    "        # Set up embedding layer with GloVe weights\n",
    "        vocab_size = embedding_matrix.shape[0]\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = trainable\n",
    "\n",
    "    def _load_glove_embeddings(self, glove_file_path, embedding_dim, pad_token, unk_token):\n",
    "        # Load GloVe embeddings and build vocabulary and embedding matrix\n",
    "        embeddings_index = {}\n",
    "        with open(glove_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = vector\n",
    "        \n",
    "        # Create word index, and initialize embedding matrix\n",
    "        word_index = {word: idx for idx, word in enumerate(embeddings_index.keys(), start=2)}\n",
    "        word_index[pad_token] = 0\n",
    "        word_index[unk_token] = 1\n",
    "\n",
    "        vocab_size = len(word_index)\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "        for word, idx in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[idx] = embedding_vector\n",
    "            elif word == unk_token:\n",
    "                embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))\n",
    "            elif word == pad_token:\n",
    "                embedding_matrix[idx] = np.zeros(embedding_dim)\n",
    "        \n",
    "        return word_index, embedding_matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform embedding lookup\n",
    "        return self.embedding(x)\n",
    "\n",
    "def test(tokenizer):\n",
    "    embedding_layer = GloveEmbedding(glove_file_path, embedding_dim=100)\n",
    "\n",
    "    # Example batch of texts\n",
    "    texts = [\"hello world this is <UNK> test\",\n",
    "            \"another example sentence for testing\",\n",
    "            \"batch processing with <PAD> and <UNK> tokens\",\n",
    "            \"Check the price of the new gadget ($199) at 'Tech-Store', \" \\\n",
    "        \"and don't forget to use the discount code 'SAVE20' for 20% off on your next purchase! \" \\\n",
    "        \"For more info, call #123 or visit www.tech-store.com & sign up.\"]\n",
    "\n",
    "    # Encode batch with tokenizer, returning list format\n",
    "    encoded_list = tokenizer.encode(texts, max_length=60, return_tensors=\"list\")\n",
    "    print(\"Encoded Batch (List):\", encoded_list)\n",
    "\n",
    "    # Encode batch with tokenizer, returning torch tensor format\n",
    "    encoded_tensor = tokenizer.encode(texts, max_length=60, return_tensors=\"pt\")\n",
    "    print(\"Encoded Batch (Torch):\", encoded_tensor)\n",
    "    print(\"Shape:\", encoded_tensor[\"input_ids\"].shape, encoded_tensor[\"attention_mask\"].shape)\n",
    "\n",
    "    # Decode batch with tokenizer\n",
    "    decoded_texts = tokenizer.decode(encoded_tensor[\"input_ids\"])\n",
    "    print(\"Decoded Batch (Tensor):\", decoded_texts)\n",
    "    \n",
    "    # Decode list format\n",
    "    decoded_texts = tokenizer.decode(encoded_list[\"input_ids\"])\n",
    "    print(\"Decoded Batch (List):\", decoded_texts)\n",
    "    \n",
    "    # Pass encoded input_ids through embedding layer\n",
    "    embedded_output = embedding_layer(encoded_tensor[\"input_ids\"])\n",
    "    print(\"Embedded Output Shape (Batch):\", embedded_output.shape)\n",
    "    print(\"Embedded Output Shape (Batch):\", embedded_output)\n",
    "    \n",
    "    # Verify alignment of embeddings\n",
    "    test_word = \"text\"  # Word to check\n",
    "    if test_word in tokenizer.word_index:\n",
    "        test_index = tokenizer.word_index[test_word]\n",
    "        \n",
    "        # Retrieve GloVe vector\n",
    "        glove_vector = torch.tensor(tokenizer.embeddings_index[test_word], dtype=torch.float32)\n",
    "        \n",
    "        # Retrieve nn.Embedding vector\n",
    "        embedding_vector = embedding_layer.embedding.weight[test_index]\n",
    "        \n",
    "        # Print and compare vectors\n",
    "        print(f\"GloVe vector for '{test_word}':\", glove_vector)\n",
    "        print(f\"Embedding vector for '{test_word}':\", embedding_vector)\n",
    "        \n",
    "        # Check if vectors align\n",
    "        if torch.allclose(glove_vector, embedding_vector, atol=1e-6):\n",
    "            print(f\"The embedding for '{test_word}' is correctly aligned with the original GloVe vector.\")\n",
    "        else:\n",
    "            print(f\"The embedding for '{test_word}' is NOT aligned with the original GloVe vector.\")\n",
    "    else:\n",
    "        print(f\"The word '{test_word}' is not in the tokenizer's vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea765057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset (\"rotten_tomatoes\")\n",
    "train_data = dataset ['train']\n",
    "val_data = dataset ['validation' ]\n",
    "test_data = dataset ['test']\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "683df961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words from GloVe embeddings.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(file_path, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "# Load the GloVe embeddings (adjust the file path as needed)\n",
    "glove_file_path = 'C:\\\\Users\\\\Lee Ming Jia\\\\Desktop\\\\GloVe\\\\glove.6B.100d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "print(f\"Loaded {len(glove_embeddings)} words from GloVe embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222f196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, glove_embeddings, max_len=60, unk_token=\"<UNK>\", pad_token=\"<PAD>\"):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.glove_embeddings = glove_embeddings\n",
    "        self.max_len = max_len\n",
    "        self.unk_token = unk_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "        # GloVe vectors for unknown and padding tokens\n",
    "        self.unk_vector = np.random.normal(size=(100,))  # Random vector for OOV words\n",
    "        self.pad_vector = np.zeros((100,))  # Zero vector for padding\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        # Tokenize text into words and convert to GloVe vectors\n",
    "        words = text.split()  # Simple whitespace tokenizer\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            word = word.lower()  # Convert to lowercase to standardize\n",
    "            if word in self.glove_embeddings:\n",
    "                vectors.append(self.glove_embeddings[word])  # Use GloVe vector for known words\n",
    "            else:\n",
    "                vectors.append(self.unk_vector)  # Use random vector for unknown words\n",
    "        return vectors\n",
    "\n",
    "    def pad_sequence(self, sequence):\n",
    "        # Pad sequence to max_len\n",
    "        if len(sequence) < self.max_len:\n",
    "            # Pad with pad_token vectors\n",
    "            sequence.extend([self.pad_vector] * (self.max_len - len(sequence)))\n",
    "        else:\n",
    "            sequence = sequence[:self.max_len]  # Trim to max_len if sequence is too long\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the text and its label for the given index\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize and pad the text\n",
    "        tokenized_text = self.tokenize_text(text)\n",
    "        padded_text = self.pad_sequence(tokenized_text)\n",
    "\n",
    "        # Convert to torch tensors (embeddings are float32, labels are long)\n",
    "        return torch.tensor(padded_text, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66161867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fb0e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Rotten Tomatoes dataset (you can replace with your dataset)\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts = train_data[\"text\"]\n",
    "train_labels = train_data[\"label\"]\n",
    "val_texts = val_data[\"text\"]\n",
    "val_labels = val_data[\"label\"]\n",
    "test_texts = test_data[\"text\"]\n",
    "test_labels = test_data[\"label\"]\n",
    "\n",
    "vocab, embedding_matrix = build_vocab(train_texts, glove_embeddings)\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = TextDataset(train_texts, train_labels, glove_embeddings, max_len=60)\n",
    "val_dataset = TextDataset(val_texts, val_labels, glove_embeddings, max_len=60)\n",
    "test_dataset = TextDataset(test_texts, test_labels, glove_embeddings, max_len=60)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bc316a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Text Shape: torch.Size([32, 60, 100])\n",
      "Labels Shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Example: iterate through the training data\n",
    "#for batch_idx, (texts, labels) in enumerate(train_loader):\n",
    "#    print(f\"Batch {batch_idx + 1}\")\n",
    "#    print(f\"Text Shape: {texts.shape}\")  # Shape: (batch_size, max_len, embedding_dim)\n",
    "#    print(f\"Labels Shape: {labels.shape}\")\n",
    "#    break  # Just show the first batch for example\n",
    "    \n",
    "    \n",
    "# Example: iterate through the training data\n",
    "for batch_idx, (texts, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(f\"Text Shape: {texts.shape}\")  # Shape: (batch_size, max_len)\n",
    "    print(f\"Labels Shape: {labels.shape}\")\n",
    "\n",
    "    # Ensure that indices are within the valid range\n",
    "    max_vocab_index = len(vocab) - 1  # The highest valid index in the vocab\n",
    "    texts = torch.clamp(texts, max=max_vocab_index)  # Clamp indices to be within the valid range\n",
    "\n",
    "    # Pass the clamped texts to the model\n",
    "    predictions = model(texts)\n",
    "\n",
    "    break  # Just show the first batch for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db83b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN(len(vocab), embedding_dim, 100, [3, 4, 5], 1, 0.5, vocab['<pad>']).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get the text and label from the batch\n",
    "        text, label = batch\n",
    "        \n",
    "        # Convert text to LongTensor for the embedding layer\n",
    "        text = text.long()\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        predictions = model(text).squeeze(1)\n",
    "        \n",
    "        # Compute loss and accuracy\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, label = batch\n",
    "            \n",
    "            # Ensure text is a LongTensor for the embedding layer\n",
    "            text = text.long()\n",
    "            \n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, label)\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0828056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure texts are within the valid index range\n",
    "max_vocab_index = len(vocab) - 1  # The highest valid index\n",
    "texts = torch.clamp(texts, max=max_vocab_index)\n",
    "\n",
    "# Ensure the tensor type is LongTensor\n",
    "texts = texts.long()\n",
    "\n",
    "# Pass the clamped texts to the model\n",
    "predictions = model(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cb06bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 87\n",
      "Embedding Layer Size: 87\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "print(f\"Embedding Layer Size: {model.embedding.num_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "949b6914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs or values greater than vocab size\n",
    "print(torch.isnan(texts).any())  # Check if there are NaNs\n",
    "print((texts >= len(vocab)).any())  # Check if any values exceed the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cfaf677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Text Shape: torch.Size([32, 60, 100])\n",
      "Labels Shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Example: iterate through the training data\n",
    "#for batch_idx, (texts, labels) in enumerate(train_loader):\n",
    "#    print(f\"Batch {batch_idx + 1}\")\n",
    "#    print(f\"Text Shape: {texts.shape}\")  # Shape: (batch_size, max_len, embedding_dim)\n",
    "#    print(f\"Labels Shape: {labels.shape}\")\n",
    "#    break  # Just show the first batch for example\n",
    "    \n",
    "    \n",
    "# Example: iterate through the training data\n",
    "for batch_idx, (texts, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(f\"Text Shape: {texts.shape}\")  # Shape: (batch_size, max_len)\n",
    "    print(f\"Labels Shape: {labels.shape}\")\n",
    "\n",
    "    # Ensure that indices are within the valid range\n",
    "    #max_vocab_index = len(vocab) - 1  # The highest valid index in the vocab\n",
    "    #texts = torch.clamp(texts, max=max_vocab_index)  # Clamp indices to be within the valid range\n",
    "    \n",
    "    # Assuming texts are indices\n",
    "    texts = texts.view(-1, texts.size(2))  # Flatten to [batch_size * sequence_length, feature_size]\n",
    "\n",
    "    # Ensure indices are within the valid range\n",
    "    texts = torch.clamp(texts, 0, max_vocab_index)\n",
    "\n",
    "    \n",
    "    # Ensure that texts are of type LongTensor (indices for embeddings)\n",
    "    texts = texts.long()\n",
    "\n",
    "\n",
    "    # Pass the clamped texts to the model\n",
    "    predictions = model(texts)\n",
    "\n",
    "    break  # Just show the first batch for example\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d02e354",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LEEMIN~1\\AppData\\Local\\Temp/ipykernel_49564/1162206706.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch: {epoch+1}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\LEEMIN~1\\AppData\\Local\\Temp/ipykernel_49564/2687539651.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Forward pass through the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Compute loss and accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\LEEMIN~1\\AppData\\Local\\Temp/ipykernel_49564/3432053744.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Apply embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Add channel dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mconved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mpooled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconved\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2549\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd583ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ae826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c145a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
