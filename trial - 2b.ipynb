{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_PATH = \"C:\\\\Users\\\\Administrator\\\\Desktop\\\\glove\\\\glove.6B.100d.txt\" # change to your path\n",
    "EMBED_DIM = 100 # No need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, f1_score, f1_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import seaborn as sns\n",
    "from Embeddings import GloveEmbedding, GloveTokenizerNoSub\n",
    "import itertools\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Any\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(114514)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tokenizer\n",
    "tokenizer = GloveTokenizerNoSub(glove_file_path=GLOVE_PATH)\n",
    "embedding = GloveEmbedding(\n",
    "    glove_file_path = GLOVE_PATH, \n",
    "    trainable = False # False for part 2 to disable the Embedding training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, embed_size, bidirectional, num_rnn_layer, dropout_rate, layer_norm):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.embedding = embedding  # pass pretrained embedding in\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True, num_layers = num_rnn_layer, bidirectional=bidirectional, dropout = dropout_rate if num_rnn_layer > 1 else 0)\n",
    "        \n",
    "        # if bidirectional, the output hidden size is double\n",
    "        self.hidden_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # MLP\n",
    "        if not layer_norm:\n",
    "            self.output_layer = nn.Sequential(\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.hidden_size, 1),  # output dimension is 1 (sigmoid is in BCELossWithLogits, so no sigmoid here. This will get better result)\n",
    "            )\n",
    "        else:\n",
    "            self.output_layer = nn.Sequential(\n",
    "                nn.LayerNorm(self.hidden_size),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.LayerNorm(self.hidden_size),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.hidden_size, 1), \n",
    "            )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        '''\n",
    "        This method is designed for biLSTM/GRU/RNN. For LSTM/GRU/RNN for only one direction, just take hidden as the rnn output.\n",
    "        RNN - hidden -> MLP --> outcome\n",
    "        '''\n",
    "        # embed the input\n",
    "        embedded = self.embedding(input_ids)  # [batch_size, seq_length, embedding_dim]\n",
    "        \n",
    "        # feed forward\n",
    "        output, hidden = self.rnn(embedded) # we don't need encoder so we only keep the output\n",
    "\n",
    "        # make attention mask from [batch_size, seq_length] to [batch_size, seq_length, 1] as the same dimension as rnn\n",
    "        attention_mask = attention_mask.unsqueeze(-1)  # [batch_size, seq_length, 1]\n",
    "\n",
    "        # use attention mask to mask the output, ignore the padding embedding vector, only add the valid embedding vector \n",
    "        masked_output = output * attention_mask\n",
    "\n",
    "        # sum up all the valid output\n",
    "        summed_output = masked_output.sum(dim=1)  # [batch_size, hidden_size]\n",
    "\n",
    "        logits = self.output_layer(summed_output)\n",
    "        \n",
    "        return logits  # The output is [batch_size, 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "\n",
    "train_df = pd.read_csv(\"./preprocessed_dataset/train.csv\").iloc[:,1:]\n",
    "validation_df = pd.read_csv(\"./preprocessed_dataset/validation.csv\").iloc[:,1:]\n",
    "test_df = pd.read_csv(\"./preprocessed_dataset/test.csv\").iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[13077,    87,     0],\n",
       "         [  199,    34,    83]]),\n",
       " 'attention_mask': tensor([[1, 1, 0],\n",
       "         [1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to use tokenizer api\n",
    "tokenizer.encode([\"hello world\", \"how are you\"], return_tensors = \"pt\") # insist to return pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper paramaters\n",
    "dropout_rate = 0.1\n",
    "num_rnn_layer = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "val_steps = 100 # compute validation error every n step\n",
    "\n",
    "num_train_epochs = 3\n",
    "\n",
    "batch_size = 32\n",
    "lr = 1e-5\n",
    "weight_decay = 1e-5\n",
    "\n",
    "\n",
    "warmup_ratio=0.1\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset and data loader\n",
    "class CustomizeDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            df : pd.DataFrame\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(\n",
    "            self, \n",
    "            index : int\n",
    "        ) -> dict:\n",
    "        \n",
    "        inputs = self.df.iloc[index,0] # 0 is text\n",
    "        label = self.df.iloc[index, 1] # 1 is label\n",
    "        \n",
    "        return {\n",
    "            \"input_str\" : inputs, # output a string\n",
    "            \"label\" : label # output a label\n",
    "        }\n",
    "        \n",
    "# collater function (used for dynamic padding to save memory)\n",
    "class Collater:\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer : GloveTokenizerNoSub\n",
    "        ) -> None:\n",
    "        self.tokenizer = tokenizer \n",
    "    \n",
    "    def __call__(\n",
    "            self,\n",
    "            instances : list # a list of string\n",
    "        ) -> Any:\n",
    "        # __call__ is for function-like object\n",
    "        input_str_list = [instance[\"input_str\"] for instance in instances]\n",
    "        input_dict = tokenizer.encode(input_str_list, return_tensors = \"pt\") # return pytorch tensor\n",
    "        input_ids = input_dict[\"input_ids\"]\n",
    "        attention_mask = input_dict[\"attention_mask\"]\n",
    "        label = [torch.tensor(instance[\"label\"], dtype = torch.int64) for instance in instances]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"label\": torch.tensor(label),\n",
    "            \"attention_mask\": attention_mask # mask the pad position\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and data loader\n",
    "collate_fn = Collater(tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "train_ds = CustomizeDataset(\n",
    "    df = train_df\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_ds,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = collate_fn,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_ds = CustomizeDataset(\n",
    "    df = validation_df\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset = val_ds,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "test_ds = CustomizeDataset(\n",
    "    df = test_df\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_ds,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:801, warm up: 80\n"
     ]
    }
   ],
   "source": [
    "# compute warmup status\n",
    "num_training_steps = num_train_epochs * len(train_loader)\n",
    "num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "print(f\"train:{num_training_steps}, warm up: {num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer, loss_fn and so on\n",
    "\n",
    "cls_model = SentimentRNN(\n",
    "    embedding = embedding, \n",
    "    hidden_size = hidden_dim, \n",
    "    embed_size = EMBED_DIM, \n",
    "    bidirectional = False, #  False for part 2\n",
    "    num_rnn_layer = num_rnn_layer,\n",
    "    dropout_rate = dropout_rate,\n",
    "    layer_norm = True # set if you need layer norm\n",
    ")\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = AdamW(\n",
    "    params = cls_model.parameters(),\n",
    "    lr = lr,\n",
    "    weight_decay = weight_decay\n",
    ")\n",
    "\n",
    "scheduler  = get_cosine_schedule_with_warmup(\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = num_warmup_steps,\n",
    "    num_training_steps = num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy for wach data loader\n",
    "\n",
    "def compute_accuracy(data_loader: DataLoader) -> float:\n",
    "    \n",
    "    cls_model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            labels = data[\"label\"].view(-1, 1).float().to(device)\n",
    "            \n",
    "            logits = cls_model(input_ids, attention_mask=attention_mask)\n",
    "     \n",
    "            predictions = (logits > 0).long()\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def compute_loss(data_loader: DataLoader) -> float:\n",
    "    total_loss = 0\n",
    "    cls_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            labels = data[\"label\"].view(-1, 1).float().to(device)\n",
    "            \n",
    "            logits = cls_model.forward(input_ids, attention_mask)\n",
    "            loss = loss_fn.forward(\n",
    "                input = logits,\n",
    "                target = labels\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.detach().cpu()\n",
    "            \n",
    "        total_loss /= len(val_loader)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(train_loss, validation_loss, min_delta, tolerance, counter):\n",
    "    positive_diff = abs(validation_loss - train_loss)\n",
    "\n",
    "    # Increment counter if no significant improvement\n",
    "    if positive_diff >= min_delta:\n",
    "        counter += 1\n",
    "        if counter >= tolerance:\n",
    "            return True, counter\n",
    "    # Ensure the function always returns both values\n",
    "    return False, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/801] train loss: [0.7464] (epoch [1/3])\n",
      "[1/801] validation loss: [0.7005] validation accuracy: [0.4981]\n",
      "[2/801] train loss: [0.7313] (epoch [1/3])\n",
      "[3/801] train loss: [0.7327] (epoch [1/3])\n",
      "[4/801] train loss: [0.7324] (epoch [1/3])\n",
      "[5/801] train loss: [0.6955] (epoch [1/3])\n",
      "[6/801] train loss: [0.7363] (epoch [1/3])\n",
      "[7/801] train loss: [0.6552] (epoch [1/3])\n",
      "[8/801] train loss: [0.7138] (epoch [1/3])\n",
      "[9/801] train loss: [0.6937] (epoch [1/3])\n",
      "[10/801] train loss: [0.7431] (epoch [1/3])\n",
      "[11/801] train loss: [0.6925] (epoch [1/3])\n",
      "[12/801] train loss: [0.6734] (epoch [1/3])\n",
      "[13/801] train loss: [0.7420] (epoch [1/3])\n",
      "[14/801] train loss: [0.7007] (epoch [1/3])\n",
      "[15/801] train loss: [0.6805] (epoch [1/3])\n",
      "[16/801] train loss: [0.6988] (epoch [1/3])\n",
      "[17/801] train loss: [0.7665] (epoch [1/3])\n",
      "[18/801] train loss: [0.6718] (epoch [1/3])\n",
      "[19/801] train loss: [0.7074] (epoch [1/3])\n",
      "[20/801] train loss: [0.7053] (epoch [1/3])\n",
      "[21/801] train loss: [0.6874] (epoch [1/3])\n",
      "[22/801] train loss: [0.7083] (epoch [1/3])\n",
      "[23/801] train loss: [0.7086] (epoch [1/3])\n",
      "[24/801] train loss: [0.7157] (epoch [1/3])\n",
      "[25/801] train loss: [0.7057] (epoch [1/3])\n",
      "[26/801] train loss: [0.7391] (epoch [1/3])\n",
      "[27/801] train loss: [0.6929] (epoch [1/3])\n",
      "[28/801] train loss: [0.6895] (epoch [1/3])\n",
      "[29/801] train loss: [0.6777] (epoch [1/3])\n",
      "[30/801] train loss: [0.7047] (epoch [1/3])\n",
      "[31/801] train loss: [0.7059] (epoch [1/3])\n",
      "[32/801] train loss: [0.7145] (epoch [1/3])\n",
      "[33/801] train loss: [0.7000] (epoch [1/3])\n",
      "[34/801] train loss: [0.7248] (epoch [1/3])\n",
      "[35/801] train loss: [0.7305] (epoch [1/3])\n",
      "[36/801] train loss: [0.7161] (epoch [1/3])\n",
      "[37/801] train loss: [0.6893] (epoch [1/3])\n",
      "[38/801] train loss: [0.7028] (epoch [1/3])\n",
      "[39/801] train loss: [0.7201] (epoch [1/3])\n",
      "[40/801] train loss: [0.7149] (epoch [1/3])\n",
      "[41/801] train loss: [0.6872] (epoch [1/3])\n",
      "[42/801] train loss: [0.6686] (epoch [1/3])\n",
      "[43/801] train loss: [0.7000] (epoch [1/3])\n",
      "[44/801] train loss: [0.6937] (epoch [1/3])\n",
      "[45/801] train loss: [0.6890] (epoch [1/3])\n",
      "[46/801] train loss: [0.6994] (epoch [1/3])\n",
      "[47/801] train loss: [0.6949] (epoch [1/3])\n",
      "[48/801] train loss: [0.6991] (epoch [1/3])\n",
      "[49/801] train loss: [0.6993] (epoch [1/3])\n",
      "[50/801] train loss: [0.7119] (epoch [1/3])\n",
      "[51/801] train loss: [0.6910] (epoch [1/3])\n",
      "[52/801] train loss: [0.7054] (epoch [1/3])\n",
      "[53/801] train loss: [0.6861] (epoch [1/3])\n",
      "[54/801] train loss: [0.6896] (epoch [1/3])\n",
      "[55/801] train loss: [0.6987] (epoch [1/3])\n",
      "[56/801] train loss: [0.6828] (epoch [1/3])\n",
      "[57/801] train loss: [0.6988] (epoch [1/3])\n",
      "[58/801] train loss: [0.6492] (epoch [1/3])\n",
      "[59/801] train loss: [0.7022] (epoch [1/3])\n",
      "[60/801] train loss: [0.7160] (epoch [1/3])\n",
      "[61/801] train loss: [0.7185] (epoch [1/3])\n",
      "[62/801] train loss: [0.7009] (epoch [1/3])\n",
      "[63/801] train loss: [0.6963] (epoch [1/3])\n",
      "[64/801] train loss: [0.6797] (epoch [1/3])\n",
      "[65/801] train loss: [0.6661] (epoch [1/3])\n",
      "[66/801] train loss: [0.7239] (epoch [1/3])\n",
      "[67/801] train loss: [0.7059] (epoch [1/3])\n",
      "[68/801] train loss: [0.6623] (epoch [1/3])\n",
      "[69/801] train loss: [0.6967] (epoch [1/3])\n",
      "[70/801] train loss: [0.6828] (epoch [1/3])\n",
      "[71/801] train loss: [0.6971] (epoch [1/3])\n",
      "[72/801] train loss: [0.6942] (epoch [1/3])\n",
      "[73/801] train loss: [0.6782] (epoch [1/3])\n",
      "[74/801] train loss: [0.6956] (epoch [1/3])\n",
      "[75/801] train loss: [0.6813] (epoch [1/3])\n",
      "[76/801] train loss: [0.6594] (epoch [1/3])\n",
      "[77/801] train loss: [0.6761] (epoch [1/3])\n",
      "[78/801] train loss: [0.6973] (epoch [1/3])\n",
      "[79/801] train loss: [0.6917] (epoch [1/3])\n",
      "[80/801] train loss: [0.6750] (epoch [1/3])\n",
      "[81/801] train loss: [0.7117] (epoch [1/3])\n",
      "[82/801] train loss: [0.6729] (epoch [1/3])\n",
      "[83/801] train loss: [0.7188] (epoch [1/3])\n",
      "[84/801] train loss: [0.6946] (epoch [1/3])\n",
      "[85/801] train loss: [0.7072] (epoch [1/3])\n",
      "[86/801] train loss: [0.6811] (epoch [1/3])\n",
      "[87/801] train loss: [0.7071] (epoch [1/3])\n",
      "[88/801] train loss: [0.6940] (epoch [1/3])\n",
      "[89/801] train loss: [0.6751] (epoch [1/3])\n",
      "[90/801] train loss: [0.6795] (epoch [1/3])\n",
      "[91/801] train loss: [0.6917] (epoch [1/3])\n",
      "[92/801] train loss: [0.7124] (epoch [1/3])\n",
      "[93/801] train loss: [0.6744] (epoch [1/3])\n",
      "[94/801] train loss: [0.6849] (epoch [1/3])\n",
      "[95/801] train loss: [0.6848] (epoch [1/3])\n",
      "[96/801] train loss: [0.6798] (epoch [1/3])\n",
      "[97/801] train loss: [0.6978] (epoch [1/3])\n",
      "[98/801] train loss: [0.6925] (epoch [1/3])\n",
      "[99/801] train loss: [0.6751] (epoch [1/3])\n",
      "[100/801] train loss: [0.6990] (epoch [1/3])\n",
      "[100/801] validation loss: [0.6803] validation accuracy: [0.6079]\n",
      "[101/801] train loss: [0.7012] (epoch [1/3])\n",
      "[102/801] train loss: [0.6973] (epoch [1/3])\n",
      "[103/801] train loss: [0.6822] (epoch [1/3])\n",
      "[104/801] train loss: [0.6921] (epoch [1/3])\n",
      "[105/801] train loss: [0.6957] (epoch [1/3])\n",
      "[106/801] train loss: [0.7024] (epoch [1/3])\n",
      "[107/801] train loss: [0.6807] (epoch [1/3])\n",
      "[108/801] train loss: [0.6887] (epoch [1/3])\n",
      "[109/801] train loss: [0.6792] (epoch [1/3])\n",
      "[110/801] train loss: [0.6862] (epoch [1/3])\n",
      "[111/801] train loss: [0.6511] (epoch [1/3])\n",
      "[112/801] train loss: [0.6899] (epoch [1/3])\n",
      "[113/801] train loss: [0.6597] (epoch [1/3])\n",
      "[114/801] train loss: [0.6727] (epoch [1/3])\n",
      "[115/801] train loss: [0.6949] (epoch [1/3])\n",
      "[116/801] train loss: [0.6872] (epoch [1/3])\n",
      "[117/801] train loss: [0.6818] (epoch [1/3])\n",
      "[118/801] train loss: [0.6880] (epoch [1/3])\n",
      "[119/801] train loss: [0.6907] (epoch [1/3])\n",
      "[120/801] train loss: [0.6802] (epoch [1/3])\n",
      "[121/801] train loss: [0.6894] (epoch [1/3])\n",
      "[122/801] train loss: [0.6842] (epoch [1/3])\n",
      "[123/801] train loss: [0.6520] (epoch [1/3])\n",
      "[124/801] train loss: [0.6657] (epoch [1/3])\n",
      "[125/801] train loss: [0.6801] (epoch [1/3])\n",
      "[126/801] train loss: [0.6827] (epoch [1/3])\n",
      "[127/801] train loss: [0.6977] (epoch [1/3])\n",
      "[128/801] train loss: [0.6826] (epoch [1/3])\n",
      "[129/801] train loss: [0.7128] (epoch [1/3])\n",
      "[130/801] train loss: [0.6812] (epoch [1/3])\n",
      "[131/801] train loss: [0.6919] (epoch [1/3])\n",
      "[132/801] train loss: [0.6993] (epoch [1/3])\n",
      "[133/801] train loss: [0.6697] (epoch [1/3])\n",
      "[134/801] train loss: [0.6431] (epoch [1/3])\n",
      "[135/801] train loss: [0.6716] (epoch [1/3])\n",
      "[136/801] train loss: [0.6913] (epoch [1/3])\n",
      "[137/801] train loss: [0.6779] (epoch [1/3])\n",
      "[138/801] train loss: [0.6660] (epoch [1/3])\n",
      "[139/801] train loss: [0.7119] (epoch [1/3])\n",
      "[140/801] train loss: [0.7038] (epoch [1/3])\n",
      "[141/801] train loss: [0.6936] (epoch [1/3])\n",
      "[142/801] train loss: [0.6756] (epoch [1/3])\n",
      "[143/801] train loss: [0.6944] (epoch [1/3])\n",
      "[144/801] train loss: [0.6830] (epoch [1/3])\n",
      "[145/801] train loss: [0.7030] (epoch [1/3])\n",
      "[146/801] train loss: [0.6681] (epoch [1/3])\n",
      "[147/801] train loss: [0.6731] (epoch [1/3])\n",
      "[148/801] train loss: [0.6880] (epoch [1/3])\n",
      "[149/801] train loss: [0.6711] (epoch [1/3])\n",
      "[150/801] train loss: [0.6836] (epoch [1/3])\n",
      "[151/801] train loss: [0.6783] (epoch [1/3])\n",
      "[152/801] train loss: [0.6688] (epoch [1/3])\n",
      "[153/801] train loss: [0.6795] (epoch [1/3])\n",
      "[154/801] train loss: [0.6717] (epoch [1/3])\n",
      "[155/801] train loss: [0.6919] (epoch [1/3])\n",
      "[156/801] train loss: [0.7175] (epoch [1/3])\n",
      "[157/801] train loss: [0.6742] (epoch [1/3])\n",
      "[158/801] train loss: [0.6606] (epoch [1/3])\n",
      "[159/801] train loss: [0.6508] (epoch [1/3])\n",
      "[160/801] train loss: [0.6611] (epoch [1/3])\n",
      "[161/801] train loss: [0.6570] (epoch [1/3])\n",
      "[162/801] train loss: [0.6960] (epoch [1/3])\n",
      "[163/801] train loss: [0.6671] (epoch [1/3])\n",
      "[164/801] train loss: [0.6839] (epoch [1/3])\n",
      "[165/801] train loss: [0.6886] (epoch [1/3])\n",
      "[166/801] train loss: [0.6877] (epoch [1/3])\n",
      "[167/801] train loss: [0.6737] (epoch [1/3])\n",
      "[168/801] train loss: [0.7003] (epoch [1/3])\n",
      "[169/801] train loss: [0.6726] (epoch [1/3])\n",
      "[170/801] train loss: [0.6779] (epoch [1/3])\n",
      "[171/801] train loss: [0.6350] (epoch [1/3])\n",
      "[172/801] train loss: [0.6711] (epoch [1/3])\n",
      "[173/801] train loss: [0.6815] (epoch [1/3])\n",
      "[174/801] train loss: [0.6862] (epoch [1/3])\n",
      "[175/801] train loss: [0.6581] (epoch [1/3])\n",
      "[176/801] train loss: [0.6755] (epoch [1/3])\n",
      "[177/801] train loss: [0.6699] (epoch [1/3])\n",
      "[178/801] train loss: [0.6688] (epoch [1/3])\n",
      "[179/801] train loss: [0.6886] (epoch [1/3])\n",
      "[180/801] train loss: [0.6598] (epoch [1/3])\n",
      "[181/801] train loss: [0.6975] (epoch [1/3])\n",
      "[182/801] train loss: [0.6621] (epoch [1/3])\n",
      "[183/801] train loss: [0.6934] (epoch [1/3])\n",
      "[184/801] train loss: [0.6643] (epoch [1/3])\n",
      "[185/801] train loss: [0.6849] (epoch [1/3])\n",
      "[186/801] train loss: [0.6498] (epoch [1/3])\n",
      "[187/801] train loss: [0.6577] (epoch [1/3])\n",
      "[188/801] train loss: [0.6764] (epoch [1/3])\n",
      "[189/801] train loss: [0.6680] (epoch [1/3])\n",
      "[190/801] train loss: [0.6855] (epoch [1/3])\n",
      "[191/801] train loss: [0.6550] (epoch [1/3])\n",
      "[192/801] train loss: [0.6958] (epoch [1/3])\n",
      "[193/801] train loss: [0.6333] (epoch [1/3])\n",
      "[194/801] train loss: [0.6590] (epoch [1/3])\n",
      "[195/801] train loss: [0.6746] (epoch [1/3])\n",
      "[196/801] train loss: [0.6895] (epoch [1/3])\n",
      "[197/801] train loss: [0.6738] (epoch [1/3])\n",
      "[198/801] train loss: [0.6707] (epoch [1/3])\n",
      "[199/801] train loss: [0.6755] (epoch [1/3])\n",
      "[200/801] train loss: [0.6607] (epoch [1/3])\n",
      "[200/801] validation loss: [0.6633] validation accuracy: [0.6642]\n",
      "[201/801] train loss: [0.6663] (epoch [1/3])\n",
      "[202/801] train loss: [0.6908] (epoch [1/3])\n",
      "[203/801] train loss: [0.6456] (epoch [1/3])\n",
      "[204/801] train loss: [0.6841] (epoch [1/3])\n",
      "[205/801] train loss: [0.6616] (epoch [1/3])\n",
      "[206/801] train loss: [0.6480] (epoch [1/3])\n",
      "[207/801] train loss: [0.6810] (epoch [1/3])\n",
      "[208/801] train loss: [0.6905] (epoch [1/3])\n",
      "[209/801] train loss: [0.6657] (epoch [1/3])\n",
      "[210/801] train loss: [0.6639] (epoch [1/3])\n",
      "[211/801] train loss: [0.6587] (epoch [1/3])\n",
      "[212/801] train loss: [0.6574] (epoch [1/3])\n",
      "[213/801] train loss: [0.6631] (epoch [1/3])\n",
      "[214/801] train loss: [0.6786] (epoch [1/3])\n",
      "[215/801] train loss: [0.6367] (epoch [1/3])\n",
      "[216/801] train loss: [0.6573] (epoch [1/3])\n",
      "[217/801] train loss: [0.6697] (epoch [1/3])\n",
      "[218/801] train loss: [0.6898] (epoch [1/3])\n",
      "[219/801] train loss: [0.6606] (epoch [1/3])\n",
      "[220/801] train loss: [0.6777] (epoch [1/3])\n",
      "[221/801] train loss: [0.6843] (epoch [1/3])\n",
      "[222/801] train loss: [0.6712] (epoch [1/3])\n",
      "[223/801] train loss: [0.6691] (epoch [1/3])\n",
      "[224/801] train loss: [0.6831] (epoch [1/3])\n",
      "[225/801] train loss: [0.6797] (epoch [1/3])\n",
      "[226/801] train loss: [0.6808] (epoch [1/3])\n",
      "[227/801] train loss: [0.6713] (epoch [1/3])\n",
      "[228/801] train loss: [0.6402] (epoch [1/3])\n",
      "[229/801] train loss: [0.6727] (epoch [1/3])\n",
      "[230/801] train loss: [0.6406] (epoch [1/3])\n",
      "[231/801] train loss: [0.6659] (epoch [1/3])\n",
      "[232/801] train loss: [0.6791] (epoch [1/3])\n",
      "[233/801] train loss: [0.6879] (epoch [1/3])\n",
      "[234/801] train loss: [0.6340] (epoch [1/3])\n",
      "[235/801] train loss: [0.6665] (epoch [1/3])\n",
      "[236/801] train loss: [0.6589] (epoch [1/3])\n",
      "[237/801] train loss: [0.6483] (epoch [1/3])\n",
      "[238/801] train loss: [0.6717] (epoch [1/3])\n",
      "[239/801] train loss: [0.6703] (epoch [1/3])\n",
      "[240/801] train loss: [0.6555] (epoch [1/3])\n",
      "[241/801] train loss: [0.6502] (epoch [1/3])\n",
      "[242/801] train loss: [0.6543] (epoch [1/3])\n",
      "[243/801] train loss: [0.6767] (epoch [1/3])\n",
      "[244/801] train loss: [0.6325] (epoch [1/3])\n",
      "[245/801] train loss: [0.6314] (epoch [1/3])\n",
      "[246/801] train loss: [0.6276] (epoch [1/3])\n",
      "[247/801] train loss: [0.6237] (epoch [1/3])\n",
      "[248/801] train loss: [0.7117] (epoch [1/3])\n",
      "[249/801] train loss: [0.6469] (epoch [1/3])\n",
      "[250/801] train loss: [0.6674] (epoch [1/3])\n",
      "[251/801] train loss: [0.6084] (epoch [1/3])\n",
      "[252/801] train loss: [0.6315] (epoch [1/3])\n",
      "[253/801] train loss: [0.6489] (epoch [1/3])\n",
      "[254/801] train loss: [0.7123] (epoch [1/3])\n",
      "[255/801] train loss: [0.6529] (epoch [1/3])\n",
      "[256/801] train loss: [0.6365] (epoch [1/3])\n",
      "[257/801] train loss: [0.6486] (epoch [1/3])\n",
      "[258/801] train loss: [0.6740] (epoch [1/3])\n",
      "[259/801] train loss: [0.6820] (epoch [1/3])\n",
      "[260/801] train loss: [0.6819] (epoch [1/3])\n",
      "[261/801] train loss: [0.6916] (epoch [1/3])\n",
      "[262/801] train loss: [0.6393] (epoch [1/3])\n",
      "[263/801] train loss: [0.6888] (epoch [1/3])\n",
      "[264/801] train loss: [0.6920] (epoch [1/3])\n",
      "[265/801] train loss: [0.6333] (epoch [1/3])\n",
      "[266/801] train loss: [0.6460] (epoch [1/3])\n",
      "[267/801] train loss: [0.6427] (epoch [1/3])\n",
      "[268/801] train loss: [0.6486] (epoch [2/3])\n",
      "[269/801] train loss: [0.6451] (epoch [2/3])\n",
      "[270/801] train loss: [0.6524] (epoch [2/3])\n",
      "[271/801] train loss: [0.6611] (epoch [2/3])\n",
      "[272/801] train loss: [0.6522] (epoch [2/3])\n",
      "[273/801] train loss: [0.6614] (epoch [2/3])\n",
      "[274/801] train loss: [0.6270] (epoch [2/3])\n",
      "[275/801] train loss: [0.6471] (epoch [2/3])\n",
      "[276/801] train loss: [0.6484] (epoch [2/3])\n",
      "[277/801] train loss: [0.6867] (epoch [2/3])\n",
      "[278/801] train loss: [0.6515] (epoch [2/3])\n",
      "[279/801] train loss: [0.6448] (epoch [2/3])\n",
      "[280/801] train loss: [0.6864] (epoch [2/3])\n",
      "[281/801] train loss: [0.6423] (epoch [2/3])\n",
      "[282/801] train loss: [0.6818] (epoch [2/3])\n",
      "[283/801] train loss: [0.6955] (epoch [2/3])\n",
      "[284/801] train loss: [0.6720] (epoch [2/3])\n",
      "[285/801] train loss: [0.6623] (epoch [2/3])\n",
      "[286/801] train loss: [0.6485] (epoch [2/3])\n",
      "[287/801] train loss: [0.6403] (epoch [2/3])\n",
      "[288/801] train loss: [0.6181] (epoch [2/3])\n",
      "[289/801] train loss: [0.6770] (epoch [2/3])\n",
      "[290/801] train loss: [0.6338] (epoch [2/3])\n",
      "[291/801] train loss: [0.6481] (epoch [2/3])\n",
      "[292/801] train loss: [0.5982] (epoch [2/3])\n",
      "[293/801] train loss: [0.6249] (epoch [2/3])\n",
      "[294/801] train loss: [0.6601] (epoch [2/3])\n",
      "[295/801] train loss: [0.7170] (epoch [2/3])\n",
      "[296/801] train loss: [0.6706] (epoch [2/3])\n",
      "[297/801] train loss: [0.6650] (epoch [2/3])\n",
      "[298/801] train loss: [0.6233] (epoch [2/3])\n",
      "[299/801] train loss: [0.6645] (epoch [2/3])\n",
      "[300/801] train loss: [0.6692] (epoch [2/3])\n",
      "[300/801] validation loss: [0.6471] validation accuracy: [0.6670]\n",
      "[301/801] train loss: [0.6223] (epoch [2/3])\n",
      "[302/801] train loss: [0.6872] (epoch [2/3])\n",
      "[303/801] train loss: [0.6786] (epoch [2/3])\n",
      "[304/801] train loss: [0.6495] (epoch [2/3])\n",
      "[305/801] train loss: [0.5897] (epoch [2/3])\n",
      "[306/801] train loss: [0.6604] (epoch [2/3])\n",
      "[307/801] train loss: [0.6714] (epoch [2/3])\n",
      "[308/801] train loss: [0.6679] (epoch [2/3])\n",
      "[309/801] train loss: [0.5800] (epoch [2/3])\n",
      "[310/801] train loss: [0.6693] (epoch [2/3])\n",
      "[311/801] train loss: [0.6694] (epoch [2/3])\n",
      "[312/801] train loss: [0.6407] (epoch [2/3])\n",
      "[313/801] train loss: [0.6420] (epoch [2/3])\n",
      "[314/801] train loss: [0.6085] (epoch [2/3])\n",
      "[315/801] train loss: [0.6425] (epoch [2/3])\n",
      "[316/801] train loss: [0.6608] (epoch [2/3])\n",
      "[317/801] train loss: [0.6677] (epoch [2/3])\n",
      "[318/801] train loss: [0.6550] (epoch [2/3])\n",
      "[319/801] train loss: [0.6530] (epoch [2/3])\n",
      "[320/801] train loss: [0.6384] (epoch [2/3])\n",
      "[321/801] train loss: [0.7076] (epoch [2/3])\n",
      "[322/801] train loss: [0.6326] (epoch [2/3])\n",
      "[323/801] train loss: [0.6634] (epoch [2/3])\n",
      "[324/801] train loss: [0.6490] (epoch [2/3])\n",
      "[325/801] train loss: [0.6674] (epoch [2/3])\n",
      "[326/801] train loss: [0.6344] (epoch [2/3])\n",
      "[327/801] train loss: [0.6448] (epoch [2/3])\n",
      "[328/801] train loss: [0.6564] (epoch [2/3])\n",
      "[329/801] train loss: [0.6417] (epoch [2/3])\n",
      "[330/801] train loss: [0.6339] (epoch [2/3])\n",
      "[331/801] train loss: [0.6720] (epoch [2/3])\n",
      "[332/801] train loss: [0.6302] (epoch [2/3])\n",
      "[333/801] train loss: [0.6106] (epoch [2/3])\n",
      "[334/801] train loss: [0.6438] (epoch [2/3])\n",
      "[335/801] train loss: [0.6570] (epoch [2/3])\n",
      "[336/801] train loss: [0.6054] (epoch [2/3])\n",
      "[337/801] train loss: [0.6557] (epoch [2/3])\n",
      "[338/801] train loss: [0.6430] (epoch [2/3])\n",
      "[339/801] train loss: [0.6456] (epoch [2/3])\n",
      "[340/801] train loss: [0.6502] (epoch [2/3])\n",
      "[341/801] train loss: [0.6577] (epoch [2/3])\n",
      "[342/801] train loss: [0.6466] (epoch [2/3])\n",
      "[343/801] train loss: [0.6624] (epoch [2/3])\n",
      "[344/801] train loss: [0.6283] (epoch [2/3])\n",
      "[345/801] train loss: [0.6252] (epoch [2/3])\n",
      "[346/801] train loss: [0.6569] (epoch [2/3])\n",
      "[347/801] train loss: [0.6399] (epoch [2/3])\n",
      "[348/801] train loss: [0.6789] (epoch [2/3])\n",
      "[349/801] train loss: [0.6395] (epoch [2/3])\n",
      "[350/801] train loss: [0.6560] (epoch [2/3])\n",
      "[351/801] train loss: [0.5920] (epoch [2/3])\n",
      "[352/801] train loss: [0.6658] (epoch [2/3])\n",
      "[353/801] train loss: [0.7027] (epoch [2/3])\n",
      "[354/801] train loss: [0.6526] (epoch [2/3])\n",
      "[355/801] train loss: [0.6323] (epoch [2/3])\n",
      "[356/801] train loss: [0.6542] (epoch [2/3])\n",
      "[357/801] train loss: [0.6120] (epoch [2/3])\n",
      "[358/801] train loss: [0.5957] (epoch [2/3])\n",
      "[359/801] train loss: [0.6166] (epoch [2/3])\n",
      "[360/801] train loss: [0.6278] (epoch [2/3])\n",
      "[361/801] train loss: [0.6422] (epoch [2/3])\n",
      "[362/801] train loss: [0.6637] (epoch [2/3])\n",
      "[363/801] train loss: [0.6305] (epoch [2/3])\n",
      "[364/801] train loss: [0.6504] (epoch [2/3])\n",
      "[365/801] train loss: [0.6144] (epoch [2/3])\n",
      "[366/801] train loss: [0.6414] (epoch [2/3])\n",
      "[367/801] train loss: [0.6531] (epoch [2/3])\n",
      "[368/801] train loss: [0.6529] (epoch [2/3])\n",
      "[369/801] train loss: [0.6407] (epoch [2/3])\n",
      "[370/801] train loss: [0.6590] (epoch [2/3])\n",
      "[371/801] train loss: [0.6968] (epoch [2/3])\n",
      "[372/801] train loss: [0.5932] (epoch [2/3])\n",
      "[373/801] train loss: [0.6144] (epoch [2/3])\n",
      "[374/801] train loss: [0.6160] (epoch [2/3])\n",
      "[375/801] train loss: [0.6511] (epoch [2/3])\n",
      "[376/801] train loss: [0.6213] (epoch [2/3])\n",
      "[377/801] train loss: [0.6702] (epoch [2/3])\n",
      "[378/801] train loss: [0.6243] (epoch [2/3])\n",
      "[379/801] train loss: [0.6135] (epoch [2/3])\n",
      "[380/801] train loss: [0.5960] (epoch [2/3])\n",
      "[381/801] train loss: [0.6326] (epoch [2/3])\n",
      "[382/801] train loss: [0.6204] (epoch [2/3])\n",
      "[383/801] train loss: [0.6296] (epoch [2/3])\n",
      "[384/801] train loss: [0.6531] (epoch [2/3])\n",
      "[385/801] train loss: [0.6830] (epoch [2/3])\n",
      "[386/801] train loss: [0.6817] (epoch [2/3])\n",
      "[387/801] train loss: [0.6730] (epoch [2/3])\n",
      "[388/801] train loss: [0.6308] (epoch [2/3])\n",
      "[389/801] train loss: [0.6174] (epoch [2/3])\n",
      "[390/801] train loss: [0.6488] (epoch [2/3])\n",
      "[391/801] train loss: [0.6986] (epoch [2/3])\n",
      "[392/801] train loss: [0.6403] (epoch [2/3])\n",
      "[393/801] train loss: [0.6728] (epoch [2/3])\n",
      "[394/801] train loss: [0.6289] (epoch [2/3])\n",
      "[395/801] train loss: [0.6555] (epoch [2/3])\n",
      "[396/801] train loss: [0.6659] (epoch [2/3])\n",
      "[397/801] train loss: [0.6950] (epoch [2/3])\n",
      "[398/801] train loss: [0.6570] (epoch [2/3])\n",
      "[399/801] train loss: [0.6214] (epoch [2/3])\n",
      "[400/801] train loss: [0.6257] (epoch [2/3])\n",
      "[400/801] validation loss: [0.6336] validation accuracy: [0.6895]\n",
      "[401/801] train loss: [0.6638] (epoch [2/3])\n",
      "[402/801] train loss: [0.6515] (epoch [2/3])\n",
      "[403/801] train loss: [0.6705] (epoch [2/3])\n",
      "[404/801] train loss: [0.6341] (epoch [2/3])\n",
      "[405/801] train loss: [0.6918] (epoch [2/3])\n",
      "[406/801] train loss: [0.6356] (epoch [2/3])\n",
      "[407/801] train loss: [0.6270] (epoch [2/3])\n",
      "[408/801] train loss: [0.6412] (epoch [2/3])\n",
      "[409/801] train loss: [0.6371] (epoch [2/3])\n",
      "[410/801] train loss: [0.6831] (epoch [2/3])\n",
      "[411/801] train loss: [0.6471] (epoch [2/3])\n",
      "[412/801] train loss: [0.6735] (epoch [2/3])\n",
      "[413/801] train loss: [0.6419] (epoch [2/3])\n",
      "[414/801] train loss: [0.6328] (epoch [2/3])\n",
      "[415/801] train loss: [0.6168] (epoch [2/3])\n",
      "[416/801] train loss: [0.6656] (epoch [2/3])\n",
      "[417/801] train loss: [0.6203] (epoch [2/3])\n",
      "[418/801] train loss: [0.6086] (epoch [2/3])\n",
      "[419/801] train loss: [0.6511] (epoch [2/3])\n",
      "[420/801] train loss: [0.6053] (epoch [2/3])\n",
      "[421/801] train loss: [0.6382] (epoch [2/3])\n",
      "[422/801] train loss: [0.6638] (epoch [2/3])\n",
      "[423/801] train loss: [0.6251] (epoch [2/3])\n",
      "[424/801] train loss: [0.6789] (epoch [2/3])\n",
      "[425/801] train loss: [0.6389] (epoch [2/3])\n",
      "[426/801] train loss: [0.6148] (epoch [2/3])\n",
      "[427/801] train loss: [0.6648] (epoch [2/3])\n",
      "[428/801] train loss: [0.6747] (epoch [2/3])\n",
      "[429/801] train loss: [0.6450] (epoch [2/3])\n",
      "[430/801] train loss: [0.6545] (epoch [2/3])\n",
      "[431/801] train loss: [0.6135] (epoch [2/3])\n",
      "[432/801] train loss: [0.6531] (epoch [2/3])\n",
      "[433/801] train loss: [0.6561] (epoch [2/3])\n",
      "[434/801] train loss: [0.6299] (epoch [2/3])\n",
      "[435/801] train loss: [0.5739] (epoch [2/3])\n",
      "[436/801] train loss: [0.6070] (epoch [2/3])\n",
      "[437/801] train loss: [0.6499] (epoch [2/3])\n",
      "[438/801] train loss: [0.6947] (epoch [2/3])\n",
      "[439/801] train loss: [0.6904] (epoch [2/3])\n",
      "[440/801] train loss: [0.6367] (epoch [2/3])\n",
      "[441/801] train loss: [0.6154] (epoch [2/3])\n",
      "[442/801] train loss: [0.6357] (epoch [2/3])\n",
      "[443/801] train loss: [0.5926] (epoch [2/3])\n",
      "[444/801] train loss: [0.5978] (epoch [2/3])\n",
      "[445/801] train loss: [0.6261] (epoch [2/3])\n",
      "[446/801] train loss: [0.5847] (epoch [2/3])\n",
      "[447/801] train loss: [0.6227] (epoch [2/3])\n",
      "[448/801] train loss: [0.6283] (epoch [2/3])\n",
      "[449/801] train loss: [0.5764] (epoch [2/3])\n",
      "[450/801] train loss: [0.5979] (epoch [2/3])\n",
      "[451/801] train loss: [0.6536] (epoch [2/3])\n",
      "[452/801] train loss: [0.7291] (epoch [2/3])\n",
      "[453/801] train loss: [0.6441] (epoch [2/3])\n",
      "[454/801] train loss: [0.6379] (epoch [2/3])\n",
      "[455/801] train loss: [0.5991] (epoch [2/3])\n",
      "[456/801] train loss: [0.6117] (epoch [2/3])\n",
      "[457/801] train loss: [0.6089] (epoch [2/3])\n",
      "[458/801] train loss: [0.5666] (epoch [2/3])\n",
      "[459/801] train loss: [0.6475] (epoch [2/3])\n",
      "[460/801] train loss: [0.6979] (epoch [2/3])\n",
      "[461/801] train loss: [0.6737] (epoch [2/3])\n",
      "[462/801] train loss: [0.6416] (epoch [2/3])\n",
      "[463/801] train loss: [0.5951] (epoch [2/3])\n",
      "[464/801] train loss: [0.6605] (epoch [2/3])\n",
      "[465/801] train loss: [0.6853] (epoch [2/3])\n",
      "[466/801] train loss: [0.7046] (epoch [2/3])\n",
      "[467/801] train loss: [0.6068] (epoch [2/3])\n",
      "[468/801] train loss: [0.6548] (epoch [2/3])\n",
      "[469/801] train loss: [0.6390] (epoch [2/3])\n",
      "[470/801] train loss: [0.6195] (epoch [2/3])\n",
      "[471/801] train loss: [0.6246] (epoch [2/3])\n",
      "[472/801] train loss: [0.6554] (epoch [2/3])\n",
      "[473/801] train loss: [0.5661] (epoch [2/3])\n",
      "[474/801] train loss: [0.7000] (epoch [2/3])\n",
      "[475/801] train loss: [0.6081] (epoch [2/3])\n",
      "[476/801] train loss: [0.6285] (epoch [2/3])\n",
      "[477/801] train loss: [0.5920] (epoch [2/3])\n",
      "[478/801] train loss: [0.6479] (epoch [2/3])\n",
      "[479/801] train loss: [0.6257] (epoch [2/3])\n",
      "[480/801] train loss: [0.6822] (epoch [2/3])\n",
      "[481/801] train loss: [0.6489] (epoch [2/3])\n",
      "[482/801] train loss: [0.6698] (epoch [2/3])\n",
      "[483/801] train loss: [0.6000] (epoch [2/3])\n",
      "[484/801] train loss: [0.6768] (epoch [2/3])\n",
      "[485/801] train loss: [0.6786] (epoch [2/3])\n",
      "[486/801] train loss: [0.5990] (epoch [2/3])\n",
      "[487/801] train loss: [0.6112] (epoch [2/3])\n",
      "[488/801] train loss: [0.5866] (epoch [2/3])\n",
      "[489/801] train loss: [0.6503] (epoch [2/3])\n",
      "[490/801] train loss: [0.6135] (epoch [2/3])\n",
      "[491/801] train loss: [0.6492] (epoch [2/3])\n",
      "[492/801] train loss: [0.6637] (epoch [2/3])\n",
      "[493/801] train loss: [0.6014] (epoch [2/3])\n",
      "[494/801] train loss: [0.6560] (epoch [2/3])\n",
      "[495/801] train loss: [0.6090] (epoch [2/3])\n",
      "[496/801] train loss: [0.6910] (epoch [2/3])\n",
      "[497/801] train loss: [0.6494] (epoch [2/3])\n",
      "[498/801] train loss: [0.5987] (epoch [2/3])\n",
      "[499/801] train loss: [0.6496] (epoch [2/3])\n",
      "[500/801] train loss: [0.5845] (epoch [2/3])\n",
      "[500/801] validation loss: [0.6254] validation accuracy: [0.6895]\n",
      "[501/801] train loss: [0.6225] (epoch [2/3])\n",
      "[502/801] train loss: [0.6077] (epoch [2/3])\n",
      "[503/801] train loss: [0.7031] (epoch [2/3])\n",
      "[504/801] train loss: [0.6445] (epoch [2/3])\n",
      "[505/801] train loss: [0.6750] (epoch [2/3])\n",
      "[506/801] train loss: [0.6384] (epoch [2/3])\n",
      "[507/801] train loss: [0.6352] (epoch [2/3])\n",
      "[508/801] train loss: [0.6497] (epoch [2/3])\n",
      "[509/801] train loss: [0.6316] (epoch [2/3])\n",
      "[510/801] train loss: [0.5887] (epoch [2/3])\n",
      "[511/801] train loss: [0.6317] (epoch [2/3])\n",
      "[512/801] train loss: [0.6008] (epoch [2/3])\n",
      "[513/801] train loss: [0.6467] (epoch [2/3])\n",
      "[514/801] train loss: [0.6132] (epoch [2/3])\n",
      "[515/801] train loss: [0.6660] (epoch [2/3])\n",
      "[516/801] train loss: [0.6382] (epoch [2/3])\n",
      "[517/801] train loss: [0.5950] (epoch [2/3])\n",
      "[518/801] train loss: [0.6381] (epoch [2/3])\n",
      "[519/801] train loss: [0.6460] (epoch [2/3])\n",
      "[520/801] train loss: [0.6879] (epoch [2/3])\n",
      "[521/801] train loss: [0.6746] (epoch [2/3])\n",
      "[522/801] train loss: [0.6625] (epoch [2/3])\n",
      "[523/801] train loss: [0.6076] (epoch [2/3])\n",
      "[524/801] train loss: [0.6032] (epoch [2/3])\n",
      "[525/801] train loss: [0.6793] (epoch [2/3])\n",
      "[526/801] train loss: [0.6379] (epoch [2/3])\n",
      "[527/801] train loss: [0.6916] (epoch [2/3])\n",
      "[528/801] train loss: [0.6650] (epoch [2/3])\n",
      "[529/801] train loss: [0.6730] (epoch [2/3])\n",
      "[530/801] train loss: [0.6037] (epoch [2/3])\n",
      "[531/801] train loss: [0.6598] (epoch [2/3])\n",
      "[532/801] train loss: [0.6059] (epoch [2/3])\n",
      "[533/801] train loss: [0.6241] (epoch [2/3])\n",
      "[534/801] train loss: [0.6534] (epoch [2/3])\n",
      "[535/801] train loss: [0.5894] (epoch [3/3])\n",
      "[536/801] train loss: [0.5995] (epoch [3/3])\n",
      "[537/801] train loss: [0.6302] (epoch [3/3])\n",
      "[538/801] train loss: [0.6330] (epoch [3/3])\n",
      "[539/801] train loss: [0.6790] (epoch [3/3])\n",
      "[540/801] train loss: [0.6479] (epoch [3/3])\n",
      "[541/801] train loss: [0.5753] (epoch [3/3])\n",
      "[542/801] train loss: [0.5646] (epoch [3/3])\n",
      "[543/801] train loss: [0.5832] (epoch [3/3])\n",
      "[544/801] train loss: [0.6328] (epoch [3/3])\n",
      "[545/801] train loss: [0.5967] (epoch [3/3])\n",
      "[546/801] train loss: [0.6557] (epoch [3/3])\n",
      "[547/801] train loss: [0.6523] (epoch [3/3])\n",
      "[548/801] train loss: [0.5881] (epoch [3/3])\n",
      "[549/801] train loss: [0.6334] (epoch [3/3])\n",
      "[550/801] train loss: [0.6126] (epoch [3/3])\n",
      "[551/801] train loss: [0.7090] (epoch [3/3])\n",
      "[552/801] train loss: [0.6622] (epoch [3/3])\n",
      "[553/801] train loss: [0.6159] (epoch [3/3])\n",
      "[554/801] train loss: [0.6470] (epoch [3/3])\n",
      "[555/801] train loss: [0.5723] (epoch [3/3])\n",
      "[556/801] train loss: [0.6913] (epoch [3/3])\n",
      "[557/801] train loss: [0.6527] (epoch [3/3])\n",
      "[558/801] train loss: [0.6472] (epoch [3/3])\n",
      "[559/801] train loss: [0.6959] (epoch [3/3])\n",
      "[560/801] train loss: [0.6603] (epoch [3/3])\n",
      "[561/801] train loss: [0.6600] (epoch [3/3])\n",
      "[562/801] train loss: [0.6762] (epoch [3/3])\n",
      "[563/801] train loss: [0.6574] (epoch [3/3])\n",
      "[564/801] train loss: [0.6075] (epoch [3/3])\n",
      "[565/801] train loss: [0.6279] (epoch [3/3])\n",
      "[566/801] train loss: [0.6204] (epoch [3/3])\n",
      "[567/801] train loss: [0.6329] (epoch [3/3])\n",
      "[568/801] train loss: [0.6481] (epoch [3/3])\n",
      "[569/801] train loss: [0.6027] (epoch [3/3])\n",
      "[570/801] train loss: [0.6569] (epoch [3/3])\n",
      "[571/801] train loss: [0.6129] (epoch [3/3])\n",
      "[572/801] train loss: [0.6233] (epoch [3/3])\n",
      "[573/801] train loss: [0.6253] (epoch [3/3])\n",
      "[574/801] train loss: [0.6150] (epoch [3/3])\n",
      "[575/801] train loss: [0.6101] (epoch [3/3])\n",
      "[576/801] train loss: [0.6379] (epoch [3/3])\n",
      "[577/801] train loss: [0.6750] (epoch [3/3])\n",
      "[578/801] train loss: [0.6149] (epoch [3/3])\n",
      "[579/801] train loss: [0.6063] (epoch [3/3])\n",
      "[580/801] train loss: [0.5969] (epoch [3/3])\n",
      "[581/801] train loss: [0.6390] (epoch [3/3])\n",
      "[582/801] train loss: [0.6093] (epoch [3/3])\n",
      "[583/801] train loss: [0.6394] (epoch [3/3])\n",
      "[584/801] train loss: [0.6104] (epoch [3/3])\n",
      "[585/801] train loss: [0.6602] (epoch [3/3])\n",
      "[586/801] train loss: [0.6154] (epoch [3/3])\n",
      "[587/801] train loss: [0.6439] (epoch [3/3])\n",
      "[588/801] train loss: [0.6655] (epoch [3/3])\n",
      "[589/801] train loss: [0.7055] (epoch [3/3])\n",
      "[590/801] train loss: [0.5670] (epoch [3/3])\n",
      "[591/801] train loss: [0.6571] (epoch [3/3])\n",
      "[592/801] train loss: [0.6417] (epoch [3/3])\n",
      "[593/801] train loss: [0.6274] (epoch [3/3])\n",
      "[594/801] train loss: [0.6128] (epoch [3/3])\n",
      "[595/801] train loss: [0.5865] (epoch [3/3])\n",
      "[596/801] train loss: [0.6641] (epoch [3/3])\n",
      "[597/801] train loss: [0.5790] (epoch [3/3])\n",
      "[598/801] train loss: [0.6407] (epoch [3/3])\n",
      "[599/801] train loss: [0.6093] (epoch [3/3])\n",
      "[600/801] train loss: [0.6759] (epoch [3/3])\n",
      "[600/801] validation loss: [0.6205] validation accuracy: [0.6979]\n",
      "[601/801] train loss: [0.6140] (epoch [3/3])\n",
      "[602/801] train loss: [0.6344] (epoch [3/3])\n",
      "[603/801] train loss: [0.6190] (epoch [3/3])\n",
      "[604/801] train loss: [0.5829] (epoch [3/3])\n",
      "[605/801] train loss: [0.5800] (epoch [3/3])\n",
      "[606/801] train loss: [0.5506] (epoch [3/3])\n",
      "[607/801] train loss: [0.6197] (epoch [3/3])\n",
      "[608/801] train loss: [0.5879] (epoch [3/3])\n",
      "[609/801] train loss: [0.5916] (epoch [3/3])\n",
      "[610/801] train loss: [0.6497] (epoch [3/3])\n",
      "[611/801] train loss: [0.6743] (epoch [3/3])\n",
      "[612/801] train loss: [0.6146] (epoch [3/3])\n",
      "[613/801] train loss: [0.6122] (epoch [3/3])\n",
      "[614/801] train loss: [0.5732] (epoch [3/3])\n",
      "[615/801] train loss: [0.6251] (epoch [3/3])\n",
      "[616/801] train loss: [0.6562] (epoch [3/3])\n",
      "[617/801] train loss: [0.6129] (epoch [3/3])\n",
      "[618/801] train loss: [0.5822] (epoch [3/3])\n",
      "[619/801] train loss: [0.5960] (epoch [3/3])\n",
      "[620/801] train loss: [0.6263] (epoch [3/3])\n",
      "[621/801] train loss: [0.5780] (epoch [3/3])\n",
      "[622/801] train loss: [0.6342] (epoch [3/3])\n",
      "[623/801] train loss: [0.5888] (epoch [3/3])\n",
      "[624/801] train loss: [0.6209] (epoch [3/3])\n",
      "[625/801] train loss: [0.6362] (epoch [3/3])\n",
      "[626/801] train loss: [0.6398] (epoch [3/3])\n",
      "[627/801] train loss: [0.6831] (epoch [3/3])\n",
      "[628/801] train loss: [0.6252] (epoch [3/3])\n",
      "[629/801] train loss: [0.6350] (epoch [3/3])\n",
      "[630/801] train loss: [0.6759] (epoch [3/3])\n",
      "[631/801] train loss: [0.6297] (epoch [3/3])\n",
      "[632/801] train loss: [0.6925] (epoch [3/3])\n",
      "[633/801] train loss: [0.6286] (epoch [3/3])\n",
      "[634/801] train loss: [0.6616] (epoch [3/3])\n",
      "[635/801] train loss: [0.6275] (epoch [3/3])\n",
      "[636/801] train loss: [0.5823] (epoch [3/3])\n",
      "[637/801] train loss: [0.6331] (epoch [3/3])\n",
      "[638/801] train loss: [0.5697] (epoch [3/3])\n",
      "[639/801] train loss: [0.6769] (epoch [3/3])\n",
      "[640/801] train loss: [0.6003] (epoch [3/3])\n",
      "[641/801] train loss: [0.5963] (epoch [3/3])\n",
      "[642/801] train loss: [0.6218] (epoch [3/3])\n",
      "[643/801] train loss: [0.6136] (epoch [3/3])\n",
      "[644/801] train loss: [0.6587] (epoch [3/3])\n",
      "[645/801] train loss: [0.6194] (epoch [3/3])\n",
      "[646/801] train loss: [0.6566] (epoch [3/3])\n",
      "[647/801] train loss: [0.6538] (epoch [3/3])\n",
      "[648/801] train loss: [0.6027] (epoch [3/3])\n",
      "[649/801] train loss: [0.5848] (epoch [3/3])\n",
      "[650/801] train loss: [0.6123] (epoch [3/3])\n",
      "[651/801] train loss: [0.6178] (epoch [3/3])\n",
      "[652/801] train loss: [0.6236] (epoch [3/3])\n",
      "[653/801] train loss: [0.6370] (epoch [3/3])\n",
      "[654/801] train loss: [0.6205] (epoch [3/3])\n",
      "[655/801] train loss: [0.6220] (epoch [3/3])\n",
      "[656/801] train loss: [0.6236] (epoch [3/3])\n",
      "[657/801] train loss: [0.6265] (epoch [3/3])\n",
      "[658/801] train loss: [0.5949] (epoch [3/3])\n",
      "[659/801] train loss: [0.6521] (epoch [3/3])\n",
      "[660/801] train loss: [0.6197] (epoch [3/3])\n",
      "[661/801] train loss: [0.6491] (epoch [3/3])\n",
      "[662/801] train loss: [0.5595] (epoch [3/3])\n",
      "[663/801] train loss: [0.6532] (epoch [3/3])\n",
      "[664/801] train loss: [0.6320] (epoch [3/3])\n",
      "[665/801] train loss: [0.7037] (epoch [3/3])\n",
      "[666/801] train loss: [0.6827] (epoch [3/3])\n",
      "[667/801] train loss: [0.6482] (epoch [3/3])\n",
      "[668/801] train loss: [0.5924] (epoch [3/3])\n",
      "[669/801] train loss: [0.6196] (epoch [3/3])\n",
      "[670/801] train loss: [0.6069] (epoch [3/3])\n",
      "[671/801] train loss: [0.6369] (epoch [3/3])\n",
      "[672/801] train loss: [0.6145] (epoch [3/3])\n",
      "[673/801] train loss: [0.6219] (epoch [3/3])\n",
      "[674/801] train loss: [0.5994] (epoch [3/3])\n",
      "[675/801] train loss: [0.6163] (epoch [3/3])\n",
      "[676/801] train loss: [0.6374] (epoch [3/3])\n",
      "[677/801] train loss: [0.6155] (epoch [3/3])\n",
      "[678/801] train loss: [0.6036] (epoch [3/3])\n",
      "[679/801] train loss: [0.5940] (epoch [3/3])\n",
      "[680/801] train loss: [0.6577] (epoch [3/3])\n",
      "[681/801] train loss: [0.6510] (epoch [3/3])\n",
      "[682/801] train loss: [0.6158] (epoch [3/3])\n",
      "[683/801] train loss: [0.6470] (epoch [3/3])\n",
      "[684/801] train loss: [0.6592] (epoch [3/3])\n",
      "[685/801] train loss: [0.5986] (epoch [3/3])\n",
      "[686/801] train loss: [0.6149] (epoch [3/3])\n",
      "[687/801] train loss: [0.6490] (epoch [3/3])\n",
      "[688/801] train loss: [0.6570] (epoch [3/3])\n",
      "[689/801] train loss: [0.6191] (epoch [3/3])\n",
      "[690/801] train loss: [0.6492] (epoch [3/3])\n",
      "[691/801] train loss: [0.6315] (epoch [3/3])\n",
      "[692/801] train loss: [0.6880] (epoch [3/3])\n",
      "[693/801] train loss: [0.6168] (epoch [3/3])\n",
      "[694/801] train loss: [0.6244] (epoch [3/3])\n",
      "[695/801] train loss: [0.6243] (epoch [3/3])\n",
      "[696/801] train loss: [0.6029] (epoch [3/3])\n",
      "[697/801] train loss: [0.5783] (epoch [3/3])\n",
      "[698/801] train loss: [0.6393] (epoch [3/3])\n",
      "[699/801] train loss: [0.6214] (epoch [3/3])\n",
      "[700/801] train loss: [0.6658] (epoch [3/3])\n",
      "[700/801] validation loss: [0.6187] validation accuracy: [0.6951]\n",
      "[701/801] train loss: [0.6033] (epoch [3/3])\n",
      "[702/801] train loss: [0.6536] (epoch [3/3])\n",
      "[703/801] train loss: [0.6298] (epoch [3/3])\n",
      "[704/801] train loss: [0.6223] (epoch [3/3])\n",
      "[705/801] train loss: [0.6434] (epoch [3/3])\n",
      "[706/801] train loss: [0.5809] (epoch [3/3])\n",
      "[707/801] train loss: [0.5931] (epoch [3/3])\n",
      "[708/801] train loss: [0.6529] (epoch [3/3])\n",
      "[709/801] train loss: [0.6092] (epoch [3/3])\n",
      "[710/801] train loss: [0.5894] (epoch [3/3])\n",
      "[711/801] train loss: [0.6541] (epoch [3/3])\n",
      "[712/801] train loss: [0.6030] (epoch [3/3])\n",
      "[713/801] train loss: [0.6567] (epoch [3/3])\n",
      "[714/801] train loss: [0.6127] (epoch [3/3])\n",
      "[715/801] train loss: [0.6535] (epoch [3/3])\n",
      "[716/801] train loss: [0.6810] (epoch [3/3])\n",
      "[717/801] train loss: [0.6439] (epoch [3/3])\n",
      "[718/801] train loss: [0.6407] (epoch [3/3])\n",
      "[719/801] train loss: [0.5997] (epoch [3/3])\n",
      "[720/801] train loss: [0.6490] (epoch [3/3])\n",
      "[721/801] train loss: [0.6602] (epoch [3/3])\n",
      "[722/801] train loss: [0.6576] (epoch [3/3])\n",
      "[723/801] train loss: [0.6144] (epoch [3/3])\n",
      "[724/801] train loss: [0.6531] (epoch [3/3])\n",
      "[725/801] train loss: [0.6026] (epoch [3/3])\n",
      "[726/801] train loss: [0.5801] (epoch [3/3])\n",
      "[727/801] train loss: [0.6519] (epoch [3/3])\n",
      "[728/801] train loss: [0.6461] (epoch [3/3])\n",
      "[729/801] train loss: [0.6528] (epoch [3/3])\n",
      "[730/801] train loss: [0.6135] (epoch [3/3])\n",
      "[731/801] train loss: [0.6088] (epoch [3/3])\n",
      "[732/801] train loss: [0.6489] (epoch [3/3])\n",
      "[733/801] train loss: [0.6192] (epoch [3/3])\n",
      "[734/801] train loss: [0.6526] (epoch [3/3])\n",
      "[735/801] train loss: [0.6823] (epoch [3/3])\n",
      "[736/801] train loss: [0.6020] (epoch [3/3])\n",
      "[737/801] train loss: [0.5666] (epoch [3/3])\n",
      "[738/801] train loss: [0.6235] (epoch [3/3])\n",
      "[739/801] train loss: [0.5908] (epoch [3/3])\n",
      "[740/801] train loss: [0.6405] (epoch [3/3])\n",
      "[741/801] train loss: [0.5729] (epoch [3/3])\n",
      "[742/801] train loss: [0.6392] (epoch [3/3])\n",
      "[743/801] train loss: [0.6145] (epoch [3/3])\n",
      "[744/801] train loss: [0.5495] (epoch [3/3])\n",
      "[745/801] train loss: [0.6025] (epoch [3/3])\n",
      "[746/801] train loss: [0.7093] (epoch [3/3])\n",
      "[747/801] train loss: [0.6452] (epoch [3/3])\n",
      "[748/801] train loss: [0.6061] (epoch [3/3])\n",
      "[749/801] train loss: [0.5850] (epoch [3/3])\n",
      "[750/801] train loss: [0.5702] (epoch [3/3])\n",
      "[751/801] train loss: [0.5752] (epoch [3/3])\n",
      "[752/801] train loss: [0.7059] (epoch [3/3])\n",
      "[753/801] train loss: [0.6384] (epoch [3/3])\n",
      "[754/801] train loss: [0.6373] (epoch [3/3])\n",
      "[755/801] train loss: [0.5885] (epoch [3/3])\n",
      "[756/801] train loss: [0.5702] (epoch [3/3])\n",
      "[757/801] train loss: [0.6112] (epoch [3/3])\n",
      "[758/801] train loss: [0.5533] (epoch [3/3])\n",
      "[759/801] train loss: [0.6466] (epoch [3/3])\n",
      "[760/801] train loss: [0.5930] (epoch [3/3])\n",
      "[761/801] train loss: [0.5804] (epoch [3/3])\n",
      "[762/801] train loss: [0.6160] (epoch [3/3])\n",
      "[763/801] train loss: [0.6069] (epoch [3/3])\n",
      "[764/801] train loss: [0.6240] (epoch [3/3])\n",
      "[765/801] train loss: [0.6930] (epoch [3/3])\n",
      "[766/801] train loss: [0.5704] (epoch [3/3])\n",
      "[767/801] train loss: [0.6280] (epoch [3/3])\n",
      "[768/801] train loss: [0.5765] (epoch [3/3])\n",
      "[769/801] train loss: [0.6128] (epoch [3/3])\n",
      "[770/801] train loss: [0.5979] (epoch [3/3])\n",
      "[771/801] train loss: [0.6073] (epoch [3/3])\n",
      "[772/801] train loss: [0.6350] (epoch [3/3])\n",
      "[773/801] train loss: [0.7000] (epoch [3/3])\n",
      "[774/801] train loss: [0.6303] (epoch [3/3])\n",
      "[775/801] train loss: [0.6190] (epoch [3/3])\n",
      "[776/801] train loss: [0.6778] (epoch [3/3])\n",
      "[777/801] train loss: [0.6425] (epoch [3/3])\n",
      "[778/801] train loss: [0.6600] (epoch [3/3])\n",
      "[779/801] train loss: [0.5963] (epoch [3/3])\n",
      "[780/801] train loss: [0.6360] (epoch [3/3])\n",
      "[781/801] train loss: [0.6835] (epoch [3/3])\n",
      "[782/801] train loss: [0.5524] (epoch [3/3])\n",
      "[783/801] train loss: [0.6389] (epoch [3/3])\n",
      "[784/801] train loss: [0.6823] (epoch [3/3])\n",
      "[785/801] train loss: [0.6481] (epoch [3/3])\n",
      "[786/801] train loss: [0.6595] (epoch [3/3])\n",
      "[787/801] train loss: [0.6099] (epoch [3/3])\n",
      "[788/801] train loss: [0.6106] (epoch [3/3])\n",
      "[789/801] train loss: [0.6224] (epoch [3/3])\n",
      "[790/801] train loss: [0.6287] (epoch [3/3])\n",
      "[791/801] train loss: [0.6510] (epoch [3/3])\n",
      "[792/801] train loss: [0.6494] (epoch [3/3])\n",
      "[793/801] train loss: [0.5941] (epoch [3/3])\n",
      "[794/801] train loss: [0.6428] (epoch [3/3])\n",
      "[795/801] train loss: [0.7148] (epoch [3/3])\n",
      "[796/801] train loss: [0.6414] (epoch [3/3])\n",
      "[797/801] train loss: [0.5915] (epoch [3/3])\n",
      "[798/801] train loss: [0.6066] (epoch [3/3])\n",
      "[799/801] train loss: [0.6071] (epoch [3/3])\n",
      "[800/801] train loss: [0.6892] (epoch [3/3])\n",
      "[800/801] validation loss: [0.6184] validation accuracy: [0.6932]\n",
      "[801/801] train loss: [0.6196] (epoch [3/3])\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "cls_model.to(device)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "step = 0\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "val_accs = []\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    \n",
    "    # train loop\n",
    "    for data in train_loader:\n",
    "        cls_model.train()\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        attention_mask = data[\"attention_mask\"].to(device)\n",
    "        label = data[\"label\"].view(-1, 1).float().to(device)\n",
    "        \n",
    "        logits = cls_model.forward(input_ids, attention_mask)\n",
    "        \n",
    "        \n",
    "        loss = loss_fn.forward(\n",
    "            input = logits,\n",
    "            target = label\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clip (important for RNN, not necessary for LSTM/GRU/conv1d)\n",
    "        torch.nn.utils.clip_grad_norm_(cls_model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        optimizer.step() # update paramater\n",
    "        scheduler.step() \n",
    "        optimizer.zero_grad() # clear gradient\n",
    "        \n",
    "        vis_loss = loss.detach().cpu()\n",
    "        \n",
    "        print(f\"[{step + 1}/{num_training_steps}] train loss: [{vis_loss:.4f}] (epoch [{epoch + 1}/{num_train_epochs}])\")\n",
    "        \n",
    "        \n",
    "        train_losses.append(vis_loss)\n",
    "        i =0 \n",
    "\n",
    "        if (step + 1) % val_steps == 0 or step == 0:\n",
    "            # compute validation every \"val_steps\"\n",
    "           \n",
    "            cls_model.eval()\n",
    "            val_loss = compute_loss(val_loader)\n",
    "            val_acc = compute_accuracy(val_loader)\n",
    "            val_accs.append(val_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            print(f\"[{step + 1}/{num_training_steps}] validation loss: [{val_loss:.4f}] validation accuracy: [{val_acc:.4f}]\")\n",
    "            # Apply early stopping\n",
    "            should_stop, counter = early_stopping(vis_loss, val_loss, min_delta=0.01, tolerance=10, counter=counter)\n",
    "            if should_stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    if should_stop:\n",
    "        break  # Exit e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial with params: (1, 128, 0.1, 0.0001, 128, 1.0, 1)\n",
      "  Validation Accuracy: 0.4971857410881801\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 128, 1.0, 2)\n",
      "  Validation Accuracy: 0.574108818011257\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 128, 1.0, 3)\n",
      "  Validation Accuracy: 0.5318949343339587\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 128, 2.0, 1)\n",
      "  Validation Accuracy: 0.49530956848030017\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 128, 2.0, 2)\n",
      "  Validation Accuracy: 0.5534709193245778\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 128, 2.0, 3)\n",
      "  Validation Accuracy: 0.5844277673545967\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 256, 1.0, 1)\n",
      "  Validation Accuracy: 0.5619136960600375\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 256, 1.0, 2)\n",
      "  Validation Accuracy: 0.5056285178236398\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 256, 1.0, 3)\n",
      "  Validation Accuracy: 0.5684803001876173\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 256, 2.0, 1)\n",
      "  Validation Accuracy: 0.4831144465290807\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 256, 2.0, 2)\n",
      "  Validation Accuracy: 0.50093808630394\n",
      "Trial with params: (1, 128, 0.1, 0.0001, 256, 2.0, 3)\n",
      "  Validation Accuracy: 0.5196998123827392\n",
      "Trial with params: (1, 128, 0.1, 0.001, 128, 1.0, 1)\n",
      "  Validation Accuracy: 0.4981238273921201\n",
      "Trial with params: (1, 128, 0.1, 0.001, 128, 1.0, 2)\n",
      "  Validation Accuracy: 0.6604127579737336\n",
      "Trial with params: (1, 128, 0.1, 0.001, 128, 1.0, 3)\n",
      "  Validation Accuracy: 0.6557223264540337\n",
      "Trial with params: (1, 128, 0.1, 0.001, 128, 2.0, 1)\n",
      "  Validation Accuracy: 0.50093808630394\n",
      "Trial with params: (1, 128, 0.1, 0.001, 128, 2.0, 2)\n",
      "  Validation Accuracy: 0.6594746716697936\n",
      "Trial with params: (1, 128, 0.1, 0.001, 128, 2.0, 3)\n",
      "  Validation Accuracy: 0.6529080675422139\n",
      "Trial with params: (1, 128, 0.1, 0.001, 256, 1.0, 1)\n",
      "  Validation Accuracy: 0.50093808630394\n",
      "Trial with params: (1, 128, 0.1, 0.001, 256, 1.0, 2)\n",
      "  Validation Accuracy: 0.49906191369606\n",
      "Trial with params: (1, 128, 0.1, 0.001, 256, 1.0, 3)\n",
      "  Validation Accuracy: 0.6604127579737336\n",
      "Trial with params: (1, 128, 0.1, 0.001, 256, 2.0, 1)\n",
      "  Validation Accuracy: 0.5046904315196998\n",
      "Trial with params: (1, 128, 0.1, 0.001, 256, 2.0, 2)\n",
      "  Validation Accuracy: 0.5028142589118199\n",
      "Trial with params: (1, 128, 0.1, 0.001, 256, 2.0, 3)\n",
      "  Validation Accuracy: 0.6566604127579737\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 128, 1.0, 1)\n",
      "  Validation Accuracy: 0.4915572232645403\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 128, 1.0, 2)\n",
      "  Validation Accuracy: 0.525328330206379\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 128, 1.0, 3)\n",
      "  Validation Accuracy: 0.5150093808630394\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 128, 2.0, 1)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 128, 2.0, 2)\n",
      "  Validation Accuracy: 0.47654784240150094\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 128, 2.0, 3)\n",
      "  Validation Accuracy: 0.5403377110694184\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 256, 1.0, 1)\n",
      "  Validation Accuracy: 0.5093808630393997\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 256, 1.0, 2)\n",
      "  Validation Accuracy: 0.5103189493433395\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 256, 1.0, 3)\n",
      "  Validation Accuracy: 0.5431519699812383\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 256, 2.0, 1)\n",
      "  Validation Accuracy: 0.49530956848030017\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 256, 2.0, 2)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 128, 0.2, 0.0001, 256, 2.0, 3)\n",
      "  Validation Accuracy: 0.5853658536585366\n",
      "Trial with params: (1, 128, 0.2, 0.001, 128, 1.0, 1)\n",
      "  Validation Accuracy: 0.5028142589118199\n",
      "Trial with params: (1, 128, 0.2, 0.001, 128, 1.0, 2)\n",
      "  Validation Accuracy: 0.6594746716697936\n",
      "Trial with params: (1, 128, 0.2, 0.001, 128, 1.0, 3)\n",
      "  Validation Accuracy: 0.6604127579737336\n",
      "Trial with params: (1, 128, 0.2, 0.001, 128, 2.0, 1)\n",
      "  Validation Accuracy: 0.50187617260788\n",
      "Trial with params: (1, 128, 0.2, 0.001, 128, 2.0, 2)\n",
      "  Validation Accuracy: 0.6575984990619137\n",
      "Trial with params: (1, 128, 0.2, 0.001, 128, 2.0, 3)\n",
      "  Validation Accuracy: 0.599437148217636\n",
      "Trial with params: (1, 128, 0.2, 0.001, 256, 1.0, 1)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 128, 0.2, 0.001, 256, 1.0, 2)\n",
      "  Validation Accuracy: 0.5046904315196998\n",
      "Trial with params: (1, 128, 0.2, 0.001, 256, 1.0, 3)\n",
      "  Validation Accuracy: 0.6425891181988743\n",
      "Trial with params: (1, 128, 0.2, 0.001, 256, 2.0, 1)\n",
      "  Validation Accuracy: 0.5150093808630394\n",
      "Trial with params: (1, 128, 0.2, 0.001, 256, 2.0, 2)\n",
      "  Validation Accuracy: 0.5159474671669794\n",
      "Trial with params: (1, 128, 0.2, 0.001, 256, 2.0, 3)\n",
      "  Validation Accuracy: 0.6622889305816135\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 128, 1.0, 1)\n",
      "  Validation Accuracy: 0.5328330206378987\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 128, 1.0, 2)\n",
      "  Validation Accuracy: 0.6013133208255159\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 128, 1.0, 3)\n",
      "  Validation Accuracy: 0.6041275797373359\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 128, 2.0, 1)\n",
      "  Validation Accuracy: 0.5028142589118199\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 128, 2.0, 2)\n",
      "  Validation Accuracy: 0.625703564727955\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 128, 2.0, 3)\n",
      "  Validation Accuracy: 0.6454033771106942\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 256, 1.0, 1)\n",
      "  Validation Accuracy: 0.50093808630394\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 256, 1.0, 2)\n",
      "  Validation Accuracy: 0.5206378986866792\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 256, 1.0, 3)\n",
      "  Validation Accuracy: 0.5956848030018762\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 256, 2.0, 1)\n",
      "  Validation Accuracy: 0.5206378986866792\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 256, 2.0, 2)\n",
      "  Validation Accuracy: 0.5037523452157598\n",
      "Trial with params: (1, 256, 0.1, 0.0001, 256, 2.0, 3)\n",
      "  Validation Accuracy: 0.6294559099437148\n",
      "Trial with params: (1, 256, 0.1, 0.001, 128, 1.0, 1)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 256, 0.1, 0.001, 128, 1.0, 2)\n",
      "  Validation Accuracy: 0.6885553470919324\n",
      "Trial with params: (1, 256, 0.1, 0.001, 128, 1.0, 3)\n",
      "  Validation Accuracy: 0.6519699812382739\n",
      "Trial with params: (1, 256, 0.1, 0.001, 128, 2.0, 1)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 256, 0.1, 0.001, 128, 2.0, 2)\n",
      "  Validation Accuracy: 0.6819887429643527\n",
      "Trial with params: (1, 256, 0.1, 0.001, 128, 2.0, 3)\n",
      "  Validation Accuracy: 0.6538461538461539\n",
      "Trial with params: (1, 256, 0.1, 0.001, 256, 1.0, 1)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 256, 0.1, 0.001, 256, 1.0, 2)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 256, 0.1, 0.001, 256, 1.0, 3)\n",
      "  Validation Accuracy: 0.6669793621013134\n",
      "Trial with params: (1, 256, 0.1, 0.001, 256, 2.0, 1)\n",
      "  Validation Accuracy: 0.50187617260788\n",
      "Trial with params: (1, 256, 0.1, 0.001, 256, 2.0, 2)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 256, 0.1, 0.001, 256, 2.0, 3)\n",
      "  Validation Accuracy: 0.6519699812382739\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 128, 1.0, 1)\n",
      "  Validation Accuracy: 0.50187617260788\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 128, 1.0, 2)\n",
      "  Validation Accuracy: 0.6031894934333959\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 128, 1.0, 3)\n",
      "  Validation Accuracy: 0.6031894934333959\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 128, 2.0, 1)\n",
      "  Validation Accuracy: 0.49906191369606\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 128, 2.0, 2)\n",
      "  Validation Accuracy: 0.5619136960600375\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 128, 2.0, 3)\n",
      "  Validation Accuracy: 0.5919324577861164\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 256, 1.0, 1)\n",
      "  Validation Accuracy: 0.5478424015009381\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 256, 1.0, 2)\n",
      "  Validation Accuracy: 0.49624765478424016\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 256, 1.0, 3)\n",
      "  Validation Accuracy: 0.6341463414634146\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 256, 2.0, 1)\n",
      "  Validation Accuracy: 0.5075046904315197\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 256, 2.0, 2)\n",
      "  Validation Accuracy: 0.49530956848030017\n",
      "Trial with params: (1, 256, 0.2, 0.0001, 256, 2.0, 3)\n",
      "  Validation Accuracy: 0.5881801125703565\n",
      "Trial with params: (1, 256, 0.2, 0.001, 128, 1.0, 1)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 256, 0.2, 0.001, 128, 1.0, 2)\n",
      "  Validation Accuracy: 0.6772983114446529\n",
      "Trial with params: (1, 256, 0.2, 0.001, 128, 1.0, 3)\n",
      "  Validation Accuracy: 0.6472795497185742\n",
      "Trial with params: (1, 256, 0.2, 0.001, 128, 2.0, 1)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 256, 0.2, 0.001, 128, 2.0, 2)\n",
      "  Validation Accuracy: 0.6547842401500938\n",
      "Trial with params: (1, 256, 0.2, 0.001, 128, 2.0, 3)\n",
      "  Validation Accuracy: 0.6726078799249531\n",
      "Trial with params: (1, 256, 0.2, 0.001, 256, 1.0, 1)\n",
      "  Validation Accuracy: 0.5121951219512195\n",
      "Trial with params: (1, 256, 0.2, 0.001, 256, 1.0, 2)\n",
      "  Validation Accuracy: 0.50093808630394\n",
      "Trial with params: (1, 256, 0.2, 0.001, 256, 1.0, 3)\n",
      "  Validation Accuracy: 0.6697936210131332\n",
      "Trial with params: (1, 256, 0.2, 0.001, 256, 2.0, 1)\n",
      "  Validation Accuracy: 0.50187617260788\n",
      "Trial with params: (1, 256, 0.2, 0.001, 256, 2.0, 2)\n",
      "  Validation Accuracy: 0.5\n",
      "Trial with params: (1, 256, 0.2, 0.001, 256, 2.0, 3)\n",
      "  Validation Accuracy: 0.6538461538461539\n",
      "Best result:\n",
      "  Validation Loss: inf\n",
      "  Validation Accuracy: 0.6885553470919324\n",
      "  Best hyperparameters: (1, 256, 0.1, 0.001, 128, 1.0, 2)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# Define hyperparameter ranges\n",
    "param_grid = {\n",
    "    \"num_rnn_layer\": range(1, 2),  # Integer range for number of RNN layers\n",
    "    \"hidden_dim\": [128, 256],  # Integer range for hidden dimension size\n",
    "    \"dropout_rate\": [0.1, 0.2],  # Continuous range for dropout rate\n",
    "    \"learning_rate\": [1e-4, 1e-3],  # Continuous range for learning rate\n",
    "    \"batch_size\": [128, 256],  # Integer range for batch size\n",
    "    \"max_grad_norm\": [1.0, 2.0],  # Continuous range for gradient clipping\n",
    "    \"num_train_epochs\": [1,2,3]  # Integer range for number of training epochs\n",
    "}\n",
    "\n",
    "# Track the best configuration\n",
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0.0\n",
    "best_params = None\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "for params in itertools.product(\n",
    "    param_grid[\"num_rnn_layer\"],\n",
    "    param_grid[\"hidden_dim\"],\n",
    "    param_grid[\"dropout_rate\"],\n",
    "    param_grid[\"learning_rate\"],\n",
    "    param_grid[\"batch_size\"],\n",
    "    param_grid[\"max_grad_norm\"],\n",
    "    param_grid[\"num_train_epochs\"]\n",
    "):\n",
    "    # Unpack hyperparameters\n",
    "    num_rnn_layer, hidden_dim, dropout_rate, learning_rate, batch_size, max_grad_norm, num_train_epochs = params\n",
    "    \n",
    "    # Initialize model with current parameters\n",
    "    cls_model = SentimentRNN(\n",
    "        embedding=embedding,\n",
    "        hidden_size=hidden_dim,\n",
    "        embed_size=EMBED_DIM,\n",
    "        bidirectional=False,\n",
    "        num_rnn_layer=num_rnn_layer,\n",
    "        dropout_rate=dropout_rate,\n",
    "        layer_norm=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(cls_model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "    \n",
    "    # DataLoader for selected batch size\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_ds,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Early stopping and metrics tracking variables\n",
    "    counter = 0\n",
    "    step = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    should_stop = False\n",
    "    early_stopped = False  # Flag to indicate early stopping for this trial\n",
    "    \n",
    "    for epoch in range(num_train_epochs):\n",
    "        # Training loop\n",
    "        for data in train_loader:\n",
    "            cls_model.train()\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            label = data[\"label\"].view(-1, 1).float().to(device)\n",
    "            \n",
    "            logits = cls_model(input_ids, attention_mask)\n",
    "            \n",
    "            loss = loss_fn(input=logits, target=label)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(cls_model.parameters(), max_norm=max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            vis_loss = loss.detach().cpu().item()\n",
    "            train_losses.append(vis_loss)\n",
    "            \n",
    "            # Validation step every `val_steps`\n",
    "            if (step + 1) % val_steps == 0 or step == 0:\n",
    "                cls_model.eval()\n",
    "                val_loss = compute_loss(val_loader)\n",
    "                val_acc = compute_accuracy(val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                val_accs.append(val_acc)\n",
    "                \n",
    "                # Early stopping check\n",
    "                should_stop, counter = early_stopping(vis_loss, val_loss, min_delta=0.5, tolerance=2, counter=counter)\n",
    "                if should_stop:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    early_stopped = True  # Mark this trial as early stopped\n",
    "                    break\n",
    "\n",
    "            step += 1\n",
    "        \n",
    "        if should_stop:\n",
    "            break  # Exit epoch loop if early stopping is triggered\n",
    "\n",
    "    # Track the best parameters and validation loss\n",
    "    current_val_accuracy = val_accs[-1] if val_accs else 0.0\n",
    "    #current_val_loss = val_loss[-1] if val_loss else 0.0\n",
    "    if current_val_accuracy > best_val_accuracy and early_stopped==False:\n",
    "        #best_val_loss = current_val_loss\n",
    "        best_val_accuracy = current_val_accuracy\n",
    "        best_params = params\n",
    "    \n",
    "    # Print the trial's results, including early stopping info\n",
    "    print(f\"Trial with params: {params}\")\n",
    "    #print(f\"  Validation Loss: {current_val_loss}\")\n",
    "    print(f\"  Validation Accuracy: {current_val_accuracy}\")\n",
    "    if early_stopped:\n",
    "        print(\"  Early stopped\")\n",
    "    \n",
    "\n",
    "# Print the best result\n",
    "print(\"Best result:\")\n",
    "print(f\"  Validation Loss: {best_val_loss}\")\n",
    "print(f\"  Validation Accuracy: {best_val_accuracy}\")\n",
    "print(\"  Best hyperparameters:\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256, 0.1, 0.001, 128, 1.0, 2)\n"
     ]
    }
   ],
   "source": [
    "##(1, 128, 0.1, 0.001, 256, 2.0, 3) --> higest accuracy\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:102, warm up: 10\n"
     ]
    }
   ],
   "source": [
    "val_steps = 100\n",
    "batch_size = 256\n",
    "dropout_rate = 0.1\n",
    "hidden_dim = 128\n",
    "learning_rate = 0.001\n",
    "num_rnn_layer = 1\n",
    "max_grad_norm = 2.0\n",
    "num_train_epochs = 3\n",
    "\n",
    "num_training_steps = num_train_epochs * len(train_loader)\n",
    "num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
    "print(f\"train:{num_training_steps}, warm up: {num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/102] train loss: [0.6938] (epoch [1/3])\n",
      "[1/102] validation loss: [0.7427] validation accuracy: [0.4991]\n",
      "[2/102] train loss: [0.7514] (epoch [1/3])\n",
      "[3/102] train loss: [0.6731] (epoch [1/3])\n",
      "[4/102] train loss: [0.6842] (epoch [1/3])\n",
      "[5/102] train loss: [0.6906] (epoch [1/3])\n",
      "[6/102] train loss: [0.6700] (epoch [1/3])\n",
      "[7/102] train loss: [0.6812] (epoch [1/3])\n",
      "[8/102] train loss: [0.6766] (epoch [1/3])\n",
      "[9/102] train loss: [0.6708] (epoch [1/3])\n",
      "[10/102] train loss: [0.6675] (epoch [1/3])\n",
      "[11/102] train loss: [0.6595] (epoch [1/3])\n",
      "[12/102] train loss: [0.6595] (epoch [1/3])\n",
      "[13/102] train loss: [0.6615] (epoch [1/3])\n",
      "[14/102] train loss: [0.6698] (epoch [1/3])\n",
      "[15/102] train loss: [0.6593] (epoch [1/3])\n",
      "[16/102] train loss: [0.6671] (epoch [1/3])\n",
      "[17/102] train loss: [0.6610] (epoch [1/3])\n",
      "[18/102] train loss: [0.6610] (epoch [1/3])\n",
      "[19/102] train loss: [0.6629] (epoch [1/3])\n",
      "[20/102] train loss: [0.6516] (epoch [1/3])\n",
      "[21/102] train loss: [0.6525] (epoch [1/3])\n",
      "[22/102] train loss: [0.6571] (epoch [1/3])\n",
      "[23/102] train loss: [0.6332] (epoch [1/3])\n",
      "[24/102] train loss: [0.6426] (epoch [1/3])\n",
      "[25/102] train loss: [0.6737] (epoch [1/3])\n",
      "[26/102] train loss: [0.6515] (epoch [1/3])\n",
      "[27/102] train loss: [0.6418] (epoch [1/3])\n",
      "[28/102] train loss: [0.6625] (epoch [1/3])\n",
      "[29/102] train loss: [0.6326] (epoch [1/3])\n",
      "[30/102] train loss: [0.6461] (epoch [1/3])\n",
      "[31/102] train loss: [0.6231] (epoch [1/3])\n",
      "[32/102] train loss: [0.6394] (epoch [1/3])\n",
      "[33/102] train loss: [0.6298] (epoch [1/3])\n",
      "[34/102] train loss: [0.6383] (epoch [1/3])\n",
      "[35/102] train loss: [0.6414] (epoch [2/3])\n",
      "[36/102] train loss: [0.6468] (epoch [2/3])\n",
      "[37/102] train loss: [0.6447] (epoch [2/3])\n",
      "[38/102] train loss: [0.6609] (epoch [2/3])\n",
      "[39/102] train loss: [0.6604] (epoch [2/3])\n",
      "[40/102] train loss: [0.6596] (epoch [2/3])\n",
      "[41/102] train loss: [0.6156] (epoch [2/3])\n",
      "[42/102] train loss: [0.6381] (epoch [2/3])\n",
      "[43/102] train loss: [0.6348] (epoch [2/3])\n",
      "[44/102] train loss: [0.6353] (epoch [2/3])\n",
      "[45/102] train loss: [0.6229] (epoch [2/3])\n",
      "[46/102] train loss: [0.6488] (epoch [2/3])\n",
      "[47/102] train loss: [0.6385] (epoch [2/3])\n",
      "[48/102] train loss: [0.6211] (epoch [2/3])\n",
      "[49/102] train loss: [0.6191] (epoch [2/3])\n",
      "[50/102] train loss: [0.6465] (epoch [2/3])\n",
      "[51/102] train loss: [0.6405] (epoch [2/3])\n",
      "[52/102] train loss: [0.6412] (epoch [2/3])\n",
      "[53/102] train loss: [0.6342] (epoch [2/3])\n",
      "[54/102] train loss: [0.6310] (epoch [2/3])\n",
      "[55/102] train loss: [0.6305] (epoch [2/3])\n",
      "[56/102] train loss: [0.6495] (epoch [2/3])\n",
      "[57/102] train loss: [0.6622] (epoch [2/3])\n",
      "[58/102] train loss: [0.6450] (epoch [2/3])\n",
      "[59/102] train loss: [0.6295] (epoch [2/3])\n",
      "[60/102] train loss: [0.6420] (epoch [2/3])\n",
      "[61/102] train loss: [0.6337] (epoch [2/3])\n",
      "[62/102] train loss: [0.6380] (epoch [2/3])\n",
      "[63/102] train loss: [0.6417] (epoch [2/3])\n",
      "[64/102] train loss: [0.6275] (epoch [2/3])\n",
      "[65/102] train loss: [0.6308] (epoch [2/3])\n",
      "[66/102] train loss: [0.6306] (epoch [2/3])\n",
      "[67/102] train loss: [0.6346] (epoch [2/3])\n",
      "[68/102] train loss: [0.6300] (epoch [2/3])\n",
      "[69/102] train loss: [0.6276] (epoch [3/3])\n",
      "[70/102] train loss: [0.6650] (epoch [3/3])\n",
      "[71/102] train loss: [0.6252] (epoch [3/3])\n",
      "[72/102] train loss: [0.6387] (epoch [3/3])\n",
      "[73/102] train loss: [0.6207] (epoch [3/3])\n",
      "[74/102] train loss: [0.6390] (epoch [3/3])\n",
      "[75/102] train loss: [0.6595] (epoch [3/3])\n",
      "[76/102] train loss: [0.6217] (epoch [3/3])\n",
      "[77/102] train loss: [0.6433] (epoch [3/3])\n",
      "[78/102] train loss: [0.6428] (epoch [3/3])\n",
      "[79/102] train loss: [0.6438] (epoch [3/3])\n",
      "[80/102] train loss: [0.6274] (epoch [3/3])\n",
      "[81/102] train loss: [0.6242] (epoch [3/3])\n",
      "[82/102] train loss: [0.6500] (epoch [3/3])\n",
      "[83/102] train loss: [0.6463] (epoch [3/3])\n",
      "[84/102] train loss: [0.6367] (epoch [3/3])\n",
      "[85/102] train loss: [0.6202] (epoch [3/3])\n",
      "[86/102] train loss: [0.6334] (epoch [3/3])\n",
      "[87/102] train loss: [0.6428] (epoch [3/3])\n",
      "[88/102] train loss: [0.6578] (epoch [3/3])\n",
      "[89/102] train loss: [0.6304] (epoch [3/3])\n",
      "[90/102] train loss: [0.6267] (epoch [3/3])\n",
      "[91/102] train loss: [0.6396] (epoch [3/3])\n",
      "[92/102] train loss: [0.6330] (epoch [3/3])\n",
      "[93/102] train loss: [0.6326] (epoch [3/3])\n",
      "[94/102] train loss: [0.6315] (epoch [3/3])\n",
      "[95/102] train loss: [0.6559] (epoch [3/3])\n",
      "[96/102] train loss: [0.6495] (epoch [3/3])\n",
      "[97/102] train loss: [0.6420] (epoch [3/3])\n",
      "[98/102] train loss: [0.6315] (epoch [3/3])\n",
      "[99/102] train loss: [0.6385] (epoch [3/3])\n",
      "[100/102] train loss: [0.6398] (epoch [3/3])\n",
      "[100/102] validation loss: [0.6403] validation accuracy: [0.6567]\n",
      "[101/102] train loss: [0.6322] (epoch [3/3])\n",
      "[102/102] train loss: [0.6301] (epoch [3/3])\n"
     ]
    }
   ],
   "source": [
    "cls_model = SentimentRNN(\n",
    "    embedding=embedding,\n",
    "    hidden_size=hidden_dim,\n",
    "    embed_size=EMBED_DIM,\n",
    "    bidirectional=False,  # False for part 2\n",
    "    num_rnn_layer=num_rnn_layer,\n",
    "    dropout_rate=dropout_rate,\n",
    "    layer_norm=True\n",
    ").to(device)\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(cls_model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "step = 0\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "val_accs = []\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    \n",
    "    # train loop\n",
    "    for data in train_loader:\n",
    "        cls_model.train()\n",
    "        input_ids = data[\"input_ids\"].to(device)\n",
    "        attention_mask = data[\"attention_mask\"].to(device)\n",
    "        label = data[\"label\"].view(-1, 1).float().to(device)\n",
    "        \n",
    "        logits = cls_model.forward(input_ids, attention_mask)\n",
    "        \n",
    "        \n",
    "        loss = loss_fn.forward(\n",
    "            input = logits,\n",
    "            target = label\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clip (important for RNN, not necessary for LSTM/GRU/conv1d)\n",
    "        torch.nn.utils.clip_grad_norm_(cls_model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        optimizer.step() # update paramater\n",
    "        scheduler.step() \n",
    "        optimizer.zero_grad() # clear gradient\n",
    "        \n",
    "        vis_loss = loss.detach().cpu()\n",
    "        \n",
    "        print(f\"[{step + 1}/{num_training_steps}] train loss: [{vis_loss:.4f}] (epoch [{epoch + 1}/{num_train_epochs}])\")\n",
    "        \n",
    "        \n",
    "        train_losses.append(vis_loss)\n",
    "        i =0 \n",
    "\n",
    "        if (step + 1) % val_steps == 0 or step == 0:\n",
    "            # compute validation every \"val_steps\"\n",
    "           \n",
    "            cls_model.eval()\n",
    "            val_loss = compute_loss(val_loader)\n",
    "            val_acc = compute_accuracy(val_loader)\n",
    "            val_accs.append(val_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            print(f\"[{step + 1}/{num_training_steps}] validation loss: [{val_loss:.4f}] validation accuracy: [{val_acc:.4f}]\")\n",
    "            # Apply early stopping\n",
    "            should_stop, counter = early_stopping(vis_loss, val_loss, min_delta=0.01, tolerance=2, counter=counter)\n",
    "            if should_stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    if should_stop:\n",
    "        break  # Exit e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: [0.6324]\n",
      "Train accuracy: [0.6547]\n",
      "Test Loss: [0.6369]\n",
      "Test accuracy: [0.6388]\n",
      "Validation Loss: [0.6403]\n",
      "Validation accuracy: [0.6567]\n"
     ]
    }
   ],
   "source": [
    "train_loss = compute_loss(train_loader)\n",
    "train_acc = compute_accuracy(train_loader)\n",
    "test_loss = compute_loss(test_loader)\n",
    "test_acc = compute_accuracy(test_loader)\n",
    "val_loss = compute_loss(val_loader)\n",
    "test_acc = compute_accuracy(test_loader)\n",
    "\n",
    "print(f\"Train Loss: [{train_loss:.4f}]\\nTrain accuracy: [{train_acc:.4f}]\")\n",
    "print(f\"Test Loss: [{test_loss:.4f}]\\nTest accuracy: [{test_acc:.4f}]\")\n",
    "print(f\"Validation Loss: [{val_loss:.4f}]\\nValidation accuracy: [{val_acc:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predictions and labels for confusion matrix computing\n",
    "def compute_pedictions_and_labels(\n",
    "        model : nn.Module, \n",
    "        test_loader : DataLoader, \n",
    "        device : torch.device\n",
    "    ) -> tuple:\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Lists to store all predictions and actual labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            labels = data[\"label\"].view(-1, 1).float().to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "     \n",
    "            predictions = (logits > 0).long()\n",
    "            \n",
    "            all_predictions.extend(predictions.view(-1).cpu().numpy())\n",
    "            all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "    \n",
    "    return all_predictions, all_labels\n",
    "\n",
    "all_predictions, all_labels = compute_pedictions_and_labels(cls_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization 2: confusion matrix\n",
    "\n",
    "def plot_confusion(\n",
    "        all_predictions : list, \n",
    "        all_labels : list\n",
    "    ) -> None:      \n",
    "            \n",
    "    cm = confusion_matrix(all_labels, all_predictions) # from sklearn\n",
    "            \n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', linewidths=0.2, cmap=\"YlGnBu\", cbar=False)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xticks([0.5, 1.5], ['Negative', 'Positive'])\n",
    "    plt.yticks([0.5, 1.5], ['Negative', 'Positive'], va='center')\n",
    "    plt.show()\n",
    "    \n",
    "plot_confusion(all_predictions, all_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
